@Comment{{
We try to keep the formatting in this bibtex file consistent. Please
try to follow the following style guide.

- Order: The entries of this file are ordered by year of appearance and
          then by the bibtex tags (newest entries at the top).
- Keys:  Use the style firstauthor.lastname + year + optional-tag.
         E.g. [Feautrier1992multi]
- '{}': Use a single pair of braces and embrace individual words/letters
         that should always remain uppercase.
- Abbreviations: Do not abbreviate conferences and journal names.
- Abstracts: Include abstracts, if available.
- ACM style: For all remaining style issues, we to follow the style used by ACM
         (see e.g., Baskaran2009)

!! The style rules are necessarily incomplete, if you would like to improve
   the style of this file, feel free to provide a patch that both extends
   the style guide and fixes the existing entries.
}}

@inproceedings{Stock2014,
 author    = {Stock, Kevin and
              Kong, Martin and
              Grosser, Tobias and
              Pouchet, Louis-No{\"e}l and
              Rastello, Fabrice and
              Ramanujam, J. and
              Sadayappan, P.},
 title     = { A Framework for Enhancing Data Reuse via Associative Reordering},
 booktitle = {Conference on Programming Language Design and Implementation (PLDI)},
 year      = {2014},
}

@inproceedings{Tavarageri2014,
 author    = {Tavarageri, Sanket and
              Krishnamoorthy, Sriram and
              Sadayappan, P.},
 title     = {Compiler-Assisted Detection of Transient Memory Errors},
 booktitle = {Conference on Programming Language Design and Implementation (PLDI)},
 year      = {2014},
}

@inproceedings{Juega2014cgo,
 author = {Juega, Carlos and P\'{e}rez, Jos\'{e} Ignacio G\'{o}mez and Tenllado, Christian and Catthoor, Francky Catthoor},
 title = {Adaptive Mapping and Parameter Selection Scheme to Improve Automatic
Code Generation for GPUs},
 booktitle = {{International Symposium on Code Generation and Optimization (CGO)}},
 year = 2014,
 address = {Orlando, FL, United States},
}

@inproceedings{Grosser2014cgo,
 author = {Grosser, Tobias and Cohen, Albert and Holewinski, Justin and Sadayappan, P. and Verdoolaege, Sven},
 title = {{Hybrid Hexagonal/Classical Tiling for GPUs}},
 booktitle = {{International Symposium on Code Generation and Optimization (CGO)}},
 year = 2014,
 address = {Orlando, FL, United States},
 url = {http://hal.inria.fr/hal-00911177}
}

@inproceedings{Jimborean2014cgo,
 author = {Jimborean, Alexandra and Koukos, Konstantinos and Spiliopoulos, Vasileios and
 Black-Schaffer, David and Kaxiras, Stefanos},
 title = {{Fix the code. Don't tweak the hardware: A new compiler approach to Voltage-Frequency scaling}},
 booktitle = {{International Symposium on Code Generation and Optimization (CGO)}},
 year = 2014,
 address = {Orlando, FL, United States},
}

@inproceedings{Venkat2014cgo,
 author = {Venkat, Anand and Shantharam, Manu and Hall, Mary and Strout, Michelle},
 title = {{Non-affine Extensions to Polyhedral Code Generation}},
 booktitle = {{International Symposium on Code Generation and Optimization (CGO)}},
 year = 2014,
 address = {Orlando, FL, United States},
}

@inproceedings{Darte2014impact,
 author = {Darte, Alain and Isoard, Alexandre},
 title = {Parametric Tiling with Inter-Tile Data Reuse},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria},
 url = {http://impact.gforge.inria.fr/impact2014/papers/impact2014-darte.pdf},
 abstract = {
  Loop tiling is a loop transformation widely used to improve spatial and
  temporal data locality, increase computation granularity, and enable blocking
  algorithms, which are particularly useful when offloading kernels on
  platforms with small memories. When hardware caches are not available, data
  transfers must be software-managed: they can be reduced by exploiting data
  reuse between tiles and, this way, avoid some useless external communications.
  An important parameter of loop tiling is the sizes of the tiles, which impact
  the size of the necessary local memory. However, for most analyzes that
  involve several tiles, which is the case for intertile data reuse, the tile
  sizes induce non-linear constraints, unless they are numerical constants.
  This complicates or prevents a parametric analysis. In this paper, we show
  that, actually, parametric tiling with inter-tile data reuse is nevertheless
  possible, i.e., it is possible to determine, at compile-time and in a
  parametric fashion, the copy-in and copy-out data sets for all tiles, with
  inter-tile reuse, as well as the sizes of the induced local memories, without
  the need to analyze the code for each tile size.
 }
}
@inproceedings{Guo2014impact,
 author = {Guo, Jing and Bernecky, Robert and
 		Thiyagalingam, Jeyarajan and Scholz, Sven-Bodo},
 title = {Polyhedral Methods for Improving Parallel Update-in-Place},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria},
 url = {http://impact.gforge.inria.fr/impact2014/papers/impact2014-guo.pdf},
 abstract = {
We demonstrate an optimization, denoted as polyhedral reuse analysis (PRA),
that uses polyhedral methods to improve the analysis of in-place update for
single-assignment arrays.  The PRA optimization attempts to determine when
parallel array operations that jointly deffine new arrays from existing ones can
reuse the memory of the existing arrays, rather than creating new ones.
Polyhedral representations and related dependency inference methods facilitate
that analysis.

In the context of SaC , we demonstrate the impact of this
optimisation using two non-trivial benchmarks evaluated on conventional shared
memory machines and on GPUs, obtaining performance improvements of 2-8 times
for LU Decomposition and of 2-10 times for Needleman-Wunsch, over the same
computations with PRA disabled.
 }
}
@inproceedings{Iooss2014impact,
 author = {Iooss, Guillaume and Rajopadhye, Sanjay and
 	Alias, Christophe and Zou, Yun},
 title = {Constant Aspect Ratio Tiling},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria},
 url = {http://impact.gforge.inria.fr/impact2014/papers/impact2014-iooss.pdf},
 abstract = {
Parametric tiling is a well-known transformation which is widely used to improve
locality, parallelism and granularity.  However, parametric tiling is also a
non-linear transformation and this prevents polyhedral analysis or further
polyhedral transformation after parametric tiling. It is therefore generally
applied during the code generation phase.

In this paper, we present a method
to remain polyhedral, in a special case of parametric tiling, where all the
dimensions are tiled and all the tile sizes are constant multiples of a single
tile size parameter. We call this Constant Aspect Ratio Tiling . We show how to
mathematically transform a polyhedron and an affine function into their tiled
counterpart, which are the two main operations needed in such transformation.
}
}
@inproceedings{Li2014impact,
 author = {Li, Peng and Pouchet, Louis-No{\"e}l and Cong, Jason},
 title = {Throughput Optimization for High-Level Synthesis
 		Using Resource Constraints},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria}
}
@inproceedings{Mullapudi2014impact,
 author = {Mullapudi, Ravi Teja and Bondhugula, Uday},
 title = {Tiling for Dynamic Scheduling},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria},
 url = {http://impact.gforge.inria.fr/impact2014/papers/impact2014-mullapudi.pdf},
 abstract = {
Tiling is a key transformation used for coarsening the granularity of
parallelism and improving locality. It is known that current state-of-the-art
compiler approaches for tiling affine loop nests make use of sufficient, i.e.,
conservative conditions for the validity of tiling.  These conservative
conditions, which are used for static scheduling, miss tiling schemes for which
the tile schedule is not easy to describe statically. However, the partial
order of the tiles can be expressed using dependence relations which can be
used for dynamic scheduling at runtime. Another set of opportunities are missed
due to the classic reason that finding valid tiling hyperplanes is often harder
than checking whether a given tiling is valid.

Though the conservative
conditions for validity of tiling have worked in practice on a large number of
codes, we show that they fail to find the desired tiling in several cases –
some of these have dependence patterns similar to real world problems and
applications. We then look at ways to improve current techniques to address
this issue. To quantify the potential of the improved techniques, we manually
tile two dynamic programming algorithms – the Floyd-Warshall algorithm, and
Zuker’s RNA secondary structure prediction and report their performance on a
shared memory multicore. Our 3-d tiled dynamically scheduled implementation of
Zuker’s algorithm outperforms an optimized multi-core implementation GTfold by
a factor of 2.38. Such a 3-d tiling was possible only by reasoning with more
precise validity conditions
}
}
@inproceedings{Simbuerger2014impact,
 author = {Simb{\"u}rger, Andreas and Gr{\"o}{\ss}liger, Armin},
 title = {On the Variety of Static Control Parts in Real-World Programs:
     from Affine via Multi-dimensional to Polynomial and Just-in-Time},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria},
 url = {http://impact.gforge.inria.fr/impact2014/papers/impact2014-simbuerger.pdf},
 abstract = {
The polyhedron model has been used successfully for automatic parallelization
of code regions with loop nests satisfying certain restrictions, so-called
static control parts. A popular implementation of this model is Polly (an
extension of the LLVM compiler), which is able to identify static control parts
in the intermediate representation of the compiler. We look at static control
parts found in 50 real-world programs from different domains. We study whether
these programs are amenable to polyhedral optimization by Polly at compile time
or at run time. We report the number of static control parts with uniform or
authorne dependences found and study extensions of the current implementation
in Polly . We consider extensions which handle multi-dimensional arrays with
parametric sizes and arrays represented by "pointer-to-pointer" constructs. In
addition, we extend the modeling capabilities of Polly to a model using
semi-algebraic sets and real algebra instead of polyhedra and linear algebra.
We do not only consider the number and size of the code regions found but
measure the share of the run time the studied programs spend in the identified
regions for each of the classes of static control parts under study.
}
}
@inproceedings{Verdoolaege2014impact,
 author = {Verdoolaege, Sven and Guelton, Serge and
 	Grosser, Tobias and Cohen, Albert},
 title = {Schedule Trees},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria},
 url = {http://impact.gforge.inria.fr/impact2014/papers/impact2014-verdoolaege.pdf},
 abstract = {
 Schedules in the polyhedral model, both those that represent the original
execution order and those produced by scheduling algorithms, naturally have the
form of a tree. Generic schedule representations proposed in the literature
encode this tree structure such that it is only implicitly available.
Following the internal representation of isl , we propose to represent
schedules as explicit trees and further extend the concept by introducing
different kinds of nodes. We compare our schedule trees to other
representations in detail and illustrate how they have been successfully used
to simplify the implementation of a non-trivial polyhedral compiler.
 }
}
@inproceedings{Wang2014impact,
 author = {Wang, Wei and Cavazos, John and Porterfield, Allan},
 title = {Energy Auto-tuning using the Polyhedral Approach},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria},
 url = {http://impact.gforge.inria.fr/impact2014/papers/impact2014-wang.pdf},
 abstract = {
As the HPC community moves into the exascale computing era, application energy
has become a big concern. Tuning for energy will be essential in the effort to
overcome the limited power envelope. How is tuning for lower energy related to
tuning for faster execution? Understanding that relationship can guide both
performance and energy tuning for exascale.  In this paper, a strong
correlation is presented between the two that allows tuning for execution to be
used as a proxy for energy tuning. We also show that polyhedral compilers can
ectively tune a realistic application for both time and energy.

For a large
number of variants of the Polybench programs and LULESH energy consumption is
strongly correlated with total execution time. Optimizations can increase the
power and energy required between variants, but the variant with minimum
execution time also has the lowest energy usage. The polyhedral framework was
also used to optimize a 2D cardiac wave propagation simulation application.
Various loop optimizations including fusion, tiling, vectorization, and
auto-parallelization, achieved a 20% speedup over the baseline OpenMP
implementation, with an equivalent reduction in energy on an Intel Sandy Bridge
system.  On an Intel Xeon Phi system, improvements as high as 21% in execution
time and 19% reduction in energy are obtained

}
}
@inproceedings{Yuki2014impact,
 author = {Yuki, Tomofumi},
 title = {Understanding {PolyBench/C} 3.2 Kernels},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria},
 url = {http://impact.gforge.inria.fr/impact2014/papers/impact2014-yuki.pdf},
 abstract = {
 In this position paper, we argue the need for more rigorous specification of
kernels in the PolyBench/C benchmark suite.  Currently, the benchmarks are
mostly specified by their implementation as C code, with a one sentence
description of what the code is supposed to do. While this is suchcient in the
context of automated loop transformation, the lack of precise specification may
have let some questionable behaviors as benchmark kernels remain unnoticed.

As
an extreme example, two kernels in PolyBench/C 3.2 exhibit parametric speed up
with respect to the problem size when its questionable properties are used.
Abusing such properties can provide arbitrary speedup, which can be some factor
of millions, potentially threatening the credibility of any experimental
evaluation using PolyBench.
 }
}

@ARTICLE{Gonzalez2013tpds,
  author =  "A. Gonzalez-Escribano and Y. Torres and J. Fresno and D. Llanos",
  title =  "An extensible system for multilevel automatic data partition and mapping",
  journal =  "IEEE Transactions on Parallel and Distributed Systems ",
  year =  "2013",
  month =  "March",
  doi="10.1109/TPDS.2013.83",
  abstract = "
Automatic data distribution is a key feature to obtain efficient
implementations from abstract and portable parallel codes. We present a highly
efficient and extensible runtime library that integrates techniques for
automatic data partition and mapping. It uses a novel approach to define an
abstract interface and a plug-in system to encapsulate different types of
regular and irregular techniques, helping to generate codes which are
independent of the exact mapping functions selected. Currently, it supports
hierarchical tiling of arrays with dense and stride domains, that allows the
implementation of both data and task parallelism using a SPMD model. It
automatically computes appropriate domain partitions for a selected virtual
topology, mapping them to available processors with static or dynamic
load-balancing techniques. Our library also allows the construction of reusable
communication patterns that efficiently exploit MPI communication capabilities.
The use of our library greatly reduces the complexity of data distribution and
communication, hiding the details of the underlying architecture. The library
can be used as an abstract layer for building generic tiling operations as
well. Our experimental results show that the use of this library allows to
achieve similar performance as carefully-implemented manual versions for
several, well-known parallel kernels and benchmarks in distributed and
multicore systems, and substantially reduces programming effort.

",
  url = "http://www.computer.org/csdl/trans/td/preprint/06482561-abs.html"
}

@ARTICLE{Fresno2013js,
  author =  "J. Fresno and A. Gonzalez-Escribano and D. Llanos",
  title =  "Extending a Hierarchical Tiling Arrays Library to Support Sparse Data Partitioning",
  journal =  "The Journal of Supercomputing",
  year =  "2013",
  volume =  "64",
  number =  "1",
  pages =  "59--68",
  month =  "April",
  doi = "10.1007/s11227-012-0757-y",
  abstract = "
Layout methods for dense and sparse data are often seen as two separate
problems with their own particular techniques. However, they are based on the
same basic concepts. This paper studies how to integrate automatic data-layout
and partition techniques for both dense and sparse data structures. In
particular, we show how to include support for sparse matrices or graphs in
Hitmap, a library for hierarchical tiling and automatic mapping of arrays. The
paper shows that it is possible to offer a unique interface to work with both
dense and sparse data structures. Thus, the programmer can use a single and
homogeneous programming style, reducing the development effort and simplifying
the use of sparse data structures in parallel computations. Our experimental
evaluation shows that this integration of techniques can be effectively done
without compromising performance.
",
  url = "http://link.springer.com/article/10.1007%2Fs11227-012-0757-y"
}

@INPROCEEDINGS{Torres2013pdpta,
  author =  "Y. Torres and A. Gonzalez-Escribano and D. Llanos",
  title =  "Automatic Run-time Mapping of Polyhedral Computations to Heterogeneous Devices with Memory-size Restrictions",
  booktitle =  "PDPTA'13 - The 2013 International Conference on Parallel and Distributed Processing Techniques and Applications",
  year =  "2013",
  volume =  "2",
  month =  "July",
  publisher =  "CSREA Press",
  isbn = "1-60132-256-9, 1-60132-257-7 (1-60132-258-5)",
  abstract = "
Tools that aim to automatically map parallel computations to heterogeneous and
hierarchical systems try to divide the whole computation in parts with
computational loads adjusted to the capabilities of the target devices. Some
parts are executed in node cores, while others are executed in accelerator
devices.  Each part requires one or more data-structure pieces that should be
allocated in the device memory during the computation.

In this paper we present a model that allows such automatic mapping tools to
transparently assign computations to heterogeneous devices with different
memory size restrictions. The model requires the programmer to specify the
access patterns of the computation threads in a simple abstract form. This
information is used at run-time to determine the second-level partition of the
computation assigned to a device, ensuring that the data pieces required by
each sub-part fit in the target device memory, and that the number of kernels
launched is minimal.  We present experimental results with a prototype
implementation of the model that works for regular polyhedral expressions. We
show how it works for different example applications and access patterns,
transparently executing big computations in devices with different memory size
restrictions.
",
  url = {http://www.infor.uva.es/~diego/docs/torres13.pdf}
}

@techreport{Grosser2013Promises,
 hal_id = {hal-00848691},
 url = {http://hal.inria.fr/hal-00848691},
 title = {{The Promises of Hybrid Hexagonal/Classical Tiling for GPU}},
 author = {Grosser, Tobias and Verdoolaege, Sven and Cohen, Albert and Sadayappan, P.},
 abstract = {
  Time-tiling is necessary for efficient execution of iterative
  stencil computations. But the usual hyper-rectangular tiles cannot
  be used because of positive/negative dependence distances along the
  stencil's spatial dimensions. Several prior efforts have addressed
  this issue. However, known techniques trade enhanced data reuse
  for other causes of inefficiency, such as unbalanced parallelism,
  redundant computations, or increased control flow overhead incompatible
  with efficient GPU execution. We explore a new path to maximize the
  effectivness of time-tiling on iterative stencil computations. Our
  approach is particularly well suited for GPUs. It does not require
  any redundant computations, it favors coalesced global-memory access
  and data reuse in shared-memory/cache, avoids thread divergence, and
  extracts a high degree of parallelism. We introduce hybrid hexagonal
  tiling, combining hexagonal tile shapes along the time (sequential)
  dimension and one spatial dimension, with classical tiling for other
  spatial dimensions. An hexagonal tile shape simultaneously enable
  parallel tile execution and reuse along the time dimension. Experimental
  results demonstrate significant performance improvements over existing
  stencil compilers.
 },
 affiliation = {PARKAS - INRIA Paris-Rocquencourt, Department of Computer
 Science and Engineering - CSE},
 type = {Rapport de recherche},
 institution = {INRIA},
 number = {RR-8339},
 year = {2013},
 month = Jul,
 pdf = {http://hal.inria.fr/hal-00848691/PDF/RR-8339.pdf},
}

@article{Verdoolaege2013PPCG,
 title = {Polyhedral parallel code generation for {CUDA}},
 author = {Verdoolaege, Sven and Juega, Juan Carlos and Cohen, Albert and
           G\'{o}mez, Jos{\'e} Ignacio and Tenllado, Christian and
           Catthoor, Francky},
 journal = {ACM Transactions on Architecture and Code Optimization},
 issue_date = {January 2013},
 volume = {9},
 number = {4},
 month = jan,
 year = {2013},
 issn = {1544-3566},
 pages = {54:1--54:23},
 doi = {10.1145/2400682.2400713},
 acmid = {2400713},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@proceedings{impact2013,
  title  = "{P}roceedings of the 3rd {I}nternational {W}orkshop on {P}olyhedral {C}ompilation {T}echniques",
  editor = "Gr{\"o}{\ss}linger, Armin and Pouchet, Louis-No{\"e}l",
  year   = 2013,
  month  = Jan,
  address = "Berlin, Germany",
  url     = "http://nbn-resolving.de/urn:nbn:de:bvb:739-opus-26930",
  note    = "http://impact.gforge.inria.fr/impact2013/"
}

@inproceedings{feld.2013.impact,
  author = "Feld, Dustin and Soddemann, Thomas and J{\"u}nger, Michael and Mallach, Sven",
  title  = "{F}acilitate {SIMD}-{C}ode-{G}eneration in the {P}olyhedral {M}odel by {H}ardware-aware {A}utomatic {C}ode-{T}ransformation",
  pages  = "45--54",
  booktitle = "{P}roceedings of the 3rd {I}nternational {W}orkshop on {P}olyhedral {C}ompilation {T}echniques",
  editor = "Gr{\"o}{\ss}linger, Armin and Pouchet, Louis-No{\"e}l",
  year   = 2013,
  month  = Jan,
  address = "Berlin, Germany",
  url = "http://impact.gforge.inria.fr/impact2013/papers/impact2013_facilitate_simd_code_generation.pdf",
  abstract = {
Although Single Instruction Multiple Data (SIMD) units are available in general
purpose processors already since the 1990s, state-of-the-art compilers are
often still not capable to fully exploit them, i.e., they may miss to achieve
the best possible performance.

We present a new hardware-aware and adaptive
loop tiling approach that is based on polyhedral transformations and explicitly
dedicated to improve on auto-vectorization. It is an extension to the tiling
algorithm implemented within the PluTo framework [4, 5]. In its default
setting, PluTo uses static tile sizes and is already capable to enable the use
of SIMD units but not primarily targeted to optimize it. We experimented with
differnt tile sizes and found a strong relationship between their choice, cache
size parameters and performance. Based on this, we designed an adaptive
procedure that speciffically tiles vectorizable loops with dynamically
calculated sizes. The blocking is automatically fitted to the amount of data
read in loop iterations, the available SIMD units and the cache sizes. The
adaptive parts are built upon straightforward calculations that are
experimentally verified and evaluated. Our results show significant
improvements in the number of instructions vectorized, cache miss rates and,
finally, running times.
  }
}

@inproceedings{yuki.2013.impact,
  author = "Yuki, Tomofumi and Rajopadhye, Sanjay",
  title  = "{M}emory {A}llocations for {T}iled {U}niform {D}ependence {P}rograms",
  pages  = "13--22",
  booktitle = "{P}roceedings of the 3rd {I}nternational {W}orkshop on {P}olyhedral {C}ompilation {T}echniques",
  editor = "Gr{\"o}{\ss}linger, Armin and Pouchet, Louis-No{\"e}l",
  year   = 2013,
  month  = Jan,
  address = "Berlin, Germany",
  url = "http://impact.gforge.inria.fr/impact2013/papers/impact2013_memory_allocations_for_tiled_uniform_dependence_programs.pdf",
  abstract = {
In this paper, we develop a series of extensions to schedule-independent
storage mapping using Quasi-Universal Occupancy Vectors (QUOVs) targeting tiled
execution of polyhedral programs. By quasi-universality, we mean that we
restrict the \universe" of the schedule to those that correspond to tiling.
This provides the following benefits: (i) the shortest QUOVs may be shorter
than the fully universal ones, (ii) the shortest QUOVs can be found without any
search, and (iii) multi-statement programs can be handled.  The resulting
storage mapping is valid for tiled execution by any tile size.
}
}

@inproceedings{fassi.2013.impact,
  author = "Fassi, Im{\`e}n and Clauss, Philippe and Kuhn, Matthieu and Slama, Yosr",
  title  = "{M}ultifor for {M}ulticore",
  pages  = "37--44",
  booktitle = "{P}roceedings of the 3rd {I}nternational {W}orkshop on {P}olyhedral {C}ompilation {T}echniques",
  editor = "Gr{\"o}{\ss}linger, Armin and Pouchet, Louis-No{\"e}l",
  year   = 2013,
  month  = Jan,
  address = "Berlin, Germany",
  url = "http://impact.gforge.inria.fr/impact2013/papers/impact2013_multifor_for_multicore.pdf",
  abstract = {
 We propose a new programming control structure called "multifor", allowing to
take advantage of parallelization models that were not naturally attainable
with the polytope model before. In a multifor-loop, several loops whose bodies
are run simultaneously can be defined. Respective iteration domains are mapped
onto each other according to a run frequency - the grain - and a relative
position - the offset -. Execution models like dataflow, stencil computations or
MapReduce can be represented onto one referential iteration domain, while still
exhibiting traditional polyhedral code analysis and transformation
opportunities. Moreover, this construct provides ways to naturally exploit
hybrid parallelization models, thus significantly improving parallelization
opportunities in the multicore era. Traditional polyhedral software tools are
used to generate the corresponding code. Additionally, a promising perspective
related to nonlinear mapping of iteration spaces is also presented, yielding to
run a loop nest inside any other one by solving the problem of inverting
"ranking Ehrhart polynomials".
  }
}

@inproceedings{verdoolaege.2013.impact,
  author = "Verdoolaege, Sven and Nikolov, Hristo and Stefanov, Todor",
  title  = "{O}n {D}emand {P}arametric {A}rray {D}ataflow {A}nalysis",
  pages  = "23--36",
  booktitle = "{P}roceedings of the 3rd {I}nternational {W}orkshop on {P}olyhedral {C}ompilation {T}echniques",
  editor = "Gr{\"o}{\ss}linger, Armin and Pouchet, Louis-No{\"e}l",
  year   = 2013,
  month  = Jan,
  address = "Berlin, Germany",
  url = "http://impact.gforge.inria.fr/impact2013/papers/impact2013_on_demand_parametric_array_dataflow_analysis.pdf",
  abstract = {
We present a novel approach for exact array data ow analysis in the presence of
constructs that are not static authorne.  The approach is similar to that of
fuzzy array data ow analysis in that it also introduces parameters that
represent information that is only available at run-time, but the parameters
have a diㄦent meaning and are analyzed before they are introduced. The
approach was motivated by our work on process networks, but should be generally
useful since fewer parameters are introduced on larger inputs. We include some
preliminary experimental results.
}
}

@inproceedings{wonnacott.2013.impact,
  author = {Wonnacott, David G. and Mills Strout, Michelle},
  title  = "{O}n the {S}calability of {L}oop {T}iling {T}echniques",
  pages  = "3--11",
  booktitle = "{P}roceedings of the 3rd {I}nternational {W}orkshop on {P}olyhedral {C}ompilation {T}echniques",
  editor = "Gr{\"o}{\ss}linger, Armin and Pouchet, Louis-No{\"e}l",
  year   = 2013,
  month  = Jan,
  address = "Berlin, Germany",
  url = "http://impact.gforge.inria.fr/impact2013/papers/impact2013_on_the_scalability_of_loop_tiling_techniques.pdf",
  abstract = {
The Polyhedral model has proven to be a valuable tool for improving memory
locality and exploiting parallelism for optimizing dense array codes. This
model is expressive enough to describe transformations of imperfectly nested
loops, and to capture a variety of program transformations, including many
approaches to loop tiling. Tools such as the highly successful PLuTo automatic
parallelizer have provided empirical confirmation of the success of
polyhedral-based optimization, through experiments in which a number of
benchmarks have been executed on machines with small- to medium-scale
parallelism.

In anticipation of ever higher degrees of parallelism, we have
explored the impact of various loop tiling strategies on the asymptotic degree
of available parallelism. In our analysis, we consider “weak scaling” as
described by Gustafson, i.e., in which the data set size grows linearly with
the number of processors available. Some, but not all, of the approaches to
tiling provide weak scaling. In particular, the tiling currently performed by
PLuTo does not scale in this sense.

In this article, we review approaches to
loop tiling in the published literature, focusing on both scalability and
implementation status. We find that fully scalable tilings are not available in
general-purpose tools, and call upon the polyhedral compilation community to
focus on questions of asymptotic scalability. Finally, we identify ongoing work
that may resolve this issue.
  }
}

@inproceedings{doerfert.2013.impact,
  author = "Doerfert, Johannes and Hammacher, Clemens and Streit, Kevin and Hack, Sebastian",
  title  = "{SP}olly: {S}peculative {O}ptimizations in the {P}olyhedral {M}odel",
  pages  = "55--60",
  booktitle = "{P}roceedings of the 3rd {I}nternational {W}orkshop on {P}olyhedral {C}ompilation {T}echniques",
  editor = "Gr{\"o}{\ss}linger, Armin and Pouchet, Louis-No{\"e}l",
  year   = 2013,
  month  = Jan,
  address = "Berlin, Germany",
  url = "http://impact.gforge.inria.fr/impact2013/papers/impact2013_spolly.pdf",
  abstract = {
The polyhedral model is only applicable to code regions that form static
control parts (SCoPs) or slight extensions thereof. To apply polyhedral
techniques to a piece of code, the compiler usually checks, by static analysis,
whether all SCoP conditions are fulfilled. However, in many codes, the compiler
fails to verify that this is the case. In this paper we investigate the
rejection causes as reported by Polly , the polyhedral optimizer of a
state-of-the-art compiler. We show that many rejections follow from the
conservative overapproximation of the employed static analyses. In SPolly , a
speculative extension of Polly, we employ the knowledge of runtime features to
supersede this overapproximation. All speculatively generated variants form
valid SCoPs and are optimizable by the facilities of Polly. Our evaluation
shows that SPolly is able to ectively widen the applicability of polyhedral
optimization. On the SPEC 2000 suite, the number of optimizable code regions is
increased by 131 percent.  In 10 out of the 31 benchmarks of the PolyBench
suite, SPolly achieves speedups of up to 11-fold as compared to plain Polly.
  }
}

@inproceedings{Zuo2013,
 author = "Zuo, Wei and Li, Peng and Chen, Deming and Pouchet, Louis-No{\"e}l and Zhong, Shunan and Cong, Jason",
 title = "Improving Polyhedral Code Generation for High-Level Synthesis",
 booktitle = "International Conference on Hardware/Software Codesign and System Synthesis",
 year = 2013,
 url = "http://vast.cs.ucla.edu/sites/default/files/publications/Improving%20Polyhedral%20Code%20Generation%20for%20High-Level%20Synthesis.pdf",
 abstract = "
High-level synthesis (HLS) tools are now capable of generating high-quality RTL
codes for a number of programs. Nevertheless, for best performance aggressive
program transformations are still required to exploit data reuse and enable
communication/computation overlap. The polyhedral compilation framework has
shown great promise in this area with the development of HLS-specific
polyhedral transformation techniques and tools. However, all these techniques
rely on polyhedral code generation to translate a schedule for the program's
operations into an actual C code that is input to the HLS tool. In this work we
study the changes to the state-of-the-art polyhedral code generator CLooG which
are required to tailor it for HLS purposes. In particular, we develop various
techniques to significantly improve resource utilization on the FPGA. We also
develop a complete technique geared towards effective code generation of
rectangularly tiled code, leading to further improvements in resource
utilization. We demonstrate our techniques on a collection of affine
benchmarks, reducing by 2x on average (up to 10x) the area used after
high-level synthesis.
"
}

@article{Khan2013cudachill,
 author    = {Malik Murtaza Khan and
              Protonu Basu and
              Gabe Rudy and
              Mary W. Hall and
              Chun Chen and
              Jacqueline Chame},
 title     = {A script-based autotuning compiler system to generate high-performance
              CUDA code},
 journal   = {ACM Transactions on Architecture and Code Optimization (TACO)},
 volume    = {9},
 number    = {4},
 year      = {2013},
 pages     = {31},
 url = "http://www.cs.utah.edu/~mhall/cs6235s13/hipeac13.pdf",
 abstract = "
This article presents a novel compiler framework for CUDA code generation. The
compiler structure is designed to support autotuning, which employs empirical
techniques to evaluate a set of alternative mappings of computation kernels and
select the mapping that obtains the best performance. This article introduces a
Transformation Strategy Generator, a meta-optimizer that generates a set of
transformation recipes, which are descriptions of the mapping of the sequential
code to parallel CUDA code. These recipes comprise a search space of possible
implementations. This system achieves performance comparable and sometimes
better than manually tuned libraries and exceeds the performance of a
state-of-the-art GPU compiler.
"
}

@inproceedings{Henretty2013stencil,
 author = {Henretty, Tom and Veras, Richard and Franchetti, Franz and Pouchet, Louis-No{\"e}l and Ramanujam, J. and Sadayappan, P.},
 title = {A stencil compiler for short-vector {SIMD} architectures},
 booktitle = {International Conference on Supercomputing (ICS)},
 publisher = {ACM},
 year = {2013},
 url = "http://www.cs.ucla.edu/~pouchet/doc/ics-article.13.pdf",
 abstract = "
Stencil computations are an integral component of applications in a
number of scientific computing domains. Short-vector SIMD instruction
sets are ubiquitous on modern processors and can be used to significantly
increase the performance of stencil computations. Traditional approaches
to optimizing stencils on these platforms have focused on either
short-vector SIMD or data locality optimizations. In this paper, we
propose a domain specific language and compiler for stencil computations
that allows specification of stencils in a concise manner and automates
both locality and short-vector SIMD optimizations, along with effective
utilization of multi-core parallelism. Loop transformations to enhance
data locality and enable load-balanced parallelism are combined with a
data layout transformation to effectively increase the performance of
stencil computations. Performance increases are demonstrated for a number
of stencils on several modern SIMD architectures.
"
}

@inproceedings{Chen2012,
 author = {Chen, Chun},
 title = {Polyhedra scanning revisited},
 booktitle = {Conference on Programming Language Design and Implementation},
 year = {2012},
 pages = {499--508},
 numpages = {10},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {polyhedra scanning, polyhedral transformations},
 url = "http://ctop.cs.utah.edu/downloads/pldi128-chen.pdf",
 abstract = "
This paper presents a new polyhedra scanning system called CodeGen+ to
address the challenge of generating high-performance code for complex iteration
spaces resulting from compiler optimization and autotuning systems. The
strength of our approach lies in two new algorithms. First, a loop overhead
removal algorithm provides precise control of trade-offs between loop overhead
and code size based on actual loop nesting depth. Second, an if-statement
simplification algorithm further reduces the number of comparisons in the code.
These algorithms combined with the expressive power of Presburger arithmetic
enable CodeGen+ to support complex optimization strategies expressed in
iteration spaces. We compare with the state-of-the-art polyhedra scanning tool
CLooG on five loop nest computations, demonstrating that CodeGen+ generates
code that is simpler and up to 1.15x faster.
"
}

@article{Grosser2012polly,
 title = {Polly -- Performing polyhedral optimizations on a low-level intermediate representation},
 author = {Grosser, Tobias and Gr{\"o}{\ss}linger, Armin and Lengauer, Christian},
 journal = {Parallel Processing Letters},
 volume = {22},
 number = {04},
 year = {2012},
 publisher = {World Scientific},
 abstract = {
The polyhedral model for loop parallelization has proved to be an effective
tool for advanced optimization and automatic parallelization of programs in
higher-level languages. Yet, to integrate such optimizations seamlessly into
production compilers, they must be performed on the compiler's internal,
low-level, intermediate representation (IR). With Polly, we present an
infrastructure for polyhedral optimizations on such an IR. We describe the
detection of program parts amenable to a polyhedral optimization (so-called
static control parts), their translation to a Z-polyhedral representation,
optimizations on this representation and the generation of optimized IR code.
Furthermore, we define an interface for connecting external optimizers and
present a novel way of using the parallelism they introduce to generate SIMD
and OpenMP code. To evaluate Polly, we compile the PolyBench 2.0 benchmarks
fully automatically with PLuTo as external optimizer and parallelizer. We can
report on significant speedups.
},
 url = {http://www.worldscientific.com/doi/abs/10.1142/S0129626412500107}
}

@InProceedings{Verdoolaege2012,
 author = {Verdoolaege, Sven and Grosser, Tobias },
 title = {Polyhedral Extraction Tool},
 booktitle = { Second International Workshop on Polyhedral Compilation Techniques (IMPACT'12)},
 address = { Paris, France},
 month = jan,
 year = {2012},
 url = "http://impact.gforge.inria.fr/impact2012/workshop_IMPACT/verdoolaege.pdf",
 abstract = "
We present a new library for extracting a polyhedral model from C source. The
library is based on clang, the LLVM C frontend, and isl, a library for
manipulating quasi-affine sets and relations. The use of clang for parsing the
C code brings advanced diagnostics and full support for C99. The use of isl
allows for an easy construction and a powerful and compact representation of
the polyhedral model. Besides allowing arbitrary piecewise quasi-affine index
expressions and conditions, the library also supports some data dependent
constructs and has special treatment for unsigned integers. The library has
been successfully used to obtain polyhedral models for use in an equivalence
checker, a tool for constructing polyhedral process networks, a parallelizer
targeting GPUs and an interactive polyhedral environment.
"
}

@InProceedings{Darte2011impact,
  author = 	 {Darte, Alain},
  title = 	 {Keynote: Approximations in the Polyhedral Model},
  booktitle =    {1st International Workshop on Polyhedral Compilation Techniques (IMPACT)},
  year = 	 {2011},
  editor = 	 {C. Alias and C. Bastoul},
  address = 	 {Chamonix, France},
  abstract = {
The polyhedral model is often viewed as limited, by nature, to a restricted
class of programs, the set of nested loops with static control, affine loop
counters, and affine accesses to arrays. But, actually, it is more than that.
It is really a "model", in the sense that it is indeed not the real life (of
programs) but it is nevertheless a useful representation that approximates it.
It is always beneficial to develop transformations and optimizations in the
polytope model with this perspective in mind, i.e., as being applicable to more
programs, more general situations, provided that these programs are
approximated in a suitable way. In this talk, I will recall several cases where
the polytope model can be used to handle approximated programs: the detection
of parallelism in loops with approximated dependences, the derivation of
optimized communications with local reuse, the reuse of memory with
lattice-based array folding, the analysis of while loops. Other important
approximations, which will not be covered, concern fuzzy data-flow analysis and
array region analysis, and the link with abstract interpretation.
}
}

@InProceedings{Pradelle2011impact,
  author = 	 {Pradelle, Benoît and Ketterlin, Alain and Clauss, Philippe},
  title = 	 {Transparent Parallelization of Binary Code},
  booktitle =    {1st International Workshop on Polyhedral Compilation Techniques (IMPACT)},
  year = 	 {2011},
  editor = 	 {C. Alias and C. Bastoul},
  address = 	 {Chamonix, France},
  abstract = {
This paper describes a system that applies automatic parallelization techniques
to binary code. The system works by raising x86-64 raw executable code to an
intermediate representation that exhibits all memory accesses and relevant
register definitions, but outlines detailed computations that are not relevant
for parallelization. It then uses an off-the-shelf polyhedral parallelizer,
firrst applying appropriate enabling transformations if necessary. The last
phase lowers the internal representation into a new executable fragment,
re-injecting low-level instructions into the transformed code. The system is
shown to leverage the power of polyhedral parallelization techniques in the
absence of source code, with performance approaching those of source-to-source
tools.
},
  url = {http://perso.ens-lyon.fr/christophe.alias/impact2011/impact-03.pdf},
}

@InProceedings{Ramakrishna2011impact,
  author = 	 {Upadrasta, Ramakrishna and Cohen, Albert},
  title = 	 {Potential and Challenges of Two-Variable-Per-Inequality Sub-Polyhedral Compilation},
  booktitle =    {1st International Workshop on Polyhedral Compilation Techniques (IMPACT)},
  year = 	 {2011},
  editor = 	 {C. Alias and C. Bastoul},
  address = 	 {Chamonix, France},
  abstract = {
We explore the potential of sub-polyhedra as a means to fight some of the
algorithmic complexity challenges in polyhedral compilation. The static
analysis community has produced a rich variety of sub-polyhedral abstractions,
trading precision for scalability for interprocedural analysis and symbolic
model-checking. In this work, we evaluate the potential of the
Two-Variable-Per-Inequality (TVPI) numerical abstract domain for polyhedral
compilation. We demonstrate that strongly polynomial-time algorithms can be
designed for the construction of complex affine transformations, with
applications to affine scheduling and partitioning, with various objectives
(parallelization, tiling, latency minimization). We identify some limitations,
questions and research directions to be explored in depth in what we think is a
new approach for the construction of scalable polyhedral compilation tools.
},
  url = {http://perso.ens-lyon.fr/christophe.alias/impact2011/impact-13.pdf}
}

@InProceedings{Konstantinidis2011impact,
  author = 	 {Athanasios Konstantinidis, Athanasios and Kelly, Paul H. J.},
  title = 	 {More Definite Results from the {P}luTo Scheduling Algorithm},
  booktitle =    {1st International Workshop on Polyhedral Compilation Techniques (IMPACT)},
  year = 	 {2011},
  editor = 	 {C. Alias and C. Bastoul},
  address = 	 {Chamonix, France},
  abstract = {
The PLuTo scheduling algorithm is a well-known algorithm for automatic
scheduling in the polyhedral compilation model.  It seeks linearly independent
affine partition mappings for each statement of a Static Control Program
(SCoP), such that total communication is minimized. In this paper we show that
this algorithm can be sensitive to the layout of the global constraint matrix
thus resulting to varying performances of our target code simply by changing
the order of the constraint coefficients. We propose a simple technique that
automatically determines the right layout for the constraints and as a result
immunizes the PLuTo scheduling algorithm from constraint ordering variations.
},
  url = {http://perso.ens-lyon.fr/christophe.alias/impact2011/impact-02.pdf},
}

@InProceedings{Verdoolaege2011impact,
  author = 	 {Verdoolaege, Sven},
  title = 	 {Counting Affine Calculator and Applications},
  booktitle =    {1st International Workshop on Polyhedral Compilation Techniques (IMPACT)},
  year = 	 {2011},
  editor = 	 {C. Alias and C. Bastoul},
  address = 	 {Chamonix, France},
  abstract = {
We present an interactive tool, called iscc, for manipulating sets and
relations of integer tuples bounded by aliasne constraints over the set
variables, parameters and existentially quantified variables. A distinguishing
feature of iscc is that it provides a cardinality operation on sets and
relations that computes a symbolic expression in terms of the parameters and
domain variables for the number of elements in the set or the image of the
relation. In particular, these expressions are piecewise quasipolynomials,
which can be further manipulated in iscc. Besides basic operations on sets and
piecewise quasipolynomials, iscc also provides an interface to code generation,
lexicographic optimization, dependence analysis, transitive closures and the
symbolic computation of upper bounds and sums of piecewise quasipolynomials
over their domains.
},
  url = {http://perso.ens-lyon.fr/christophe.alias/impact2011/impact-05.pdf}
}

@InProceedings{Amini2011impact,
  author = 	 {Amini, Mehdi and Ancourt, Corinne and Coelho, Fabien and Creusillet, B{\'e}atrice and Guelton, Serge and Irigoin, François and Jouvelot, Pierre and Keryell, Ronan and Villalon, Pierre},
  title = 	 {{PIPS} Is not (just) Polyhedral Software},
  booktitle =    {1st International Workshop on Polyhedral Compilation Techniques (IMPACT)},
  year = 	 {2011},
  editor = 	 {C. Alias and C. Bastoul},
  address = 	 {Chamonix, France},
  abstract = {
Parallel and heterogeneous computing are growing in audience thanks to the
increased performance brought by ubiquitous manycores and GPU s. However,
available programming models, like OPENCL or CUDA, are far from being
straightforward to use. As a consequence, several automated or semi-automated
approaches have been proposed to automatically generate hardware-level codes
from high-level sequential sources.

Polyhedral models are becoming more popular because of their combination of
expressiveness, compactness, and accurate abstraction of the data-parallel
behaviour of programs. These models provide automatic or semi-automatic
parallelization and code transformation capabilities that target such modern
parallel architectures.

PIPS is a quarter-century old source-to-source transformation framework that
initially targeted parallel machines but then evolved to include other targets.
PIPS uses abstract interpretation on an integer polyhedral lattice to represent
program code, allowing linear relation analysis on integer variables in an
interprocedural way. The same representation is used for the dependence test
and the convex array region analysis. The polyhedral model is also more
classically used to schedule code from linear constraints.

In this paper, we illustrate the features of this compiler infrastructure on an
hypothetical input code, demonstrating the combination of polyhedral and non
polyhedral transformations.  PIPS interprocedural polyhedral analyses are used
to generate data transfers and are combined with non-polyhedral transformations
to achieve efficient CUDA code generation.
},
  url = {http://perso.ens-lyon.fr/christophe.alias/impact2011/impact-09.pdf},
}

@InProceedings{Grosser2011impact,
  author = 	 {Grosser, Tobias and Zheng, Hongbin and Aloor, Ragesh and Simb{\"u}rger, Andreas  and Gr{\"o}{\ss}linger, Armin and Pouchet, Louis-No{\"e}l},
  title = 	 {Polly -- Polyhedral Optimization in {LLVM}},
  booktitle =    {1st International Workshop on Polyhedral Compilation Techniques (IMPACT)},
  year = 	 {2011},
  editor = 	 {C. Alias and C. Bastoul},
  address = 	 {Chamonix, France},
  abstract = {
Various powerful polyhedral techniques exist to optimize computation intensive
programs ectively. Applying these techniques on any non-trivial program is
still surprisingly dincult and often not as ective as expected. Most polyhedral
tools are limited to a specific programming language.  Even for this language,
relevant code needs to match specific syntax that rarely appears in existing
code. It is therefore hard or even impossible to process existing programs
automatically. In addition, most tools target C or OpenCL code, which prevents
ective communication with compiler internal optimizers. As a result target
architecture specific optimizations are either little ective or not approached
at all.

In this paper we present Polly, a project to enable polyhedral optimizations in
LLVM. Polly automatically detects and transforms relevant program parts in a
language-independent and syntactically transparent way. Therefore, it supports
programs written in most common programming languages and constructs like C++
iterators, goto based loops and pointer arithmetic. Internally it provides a
state-of-the-art polyhedral library with full support for Z -polyhedra,
advanced data dependency analysis and support for external optimizers. Polly
includes integrated SIMD and OpenMP code generation. Through LLVM, machine code
for CPUs and GPU accelerators, C source code and even hardware descriptions can
be targeted.
},
  url = {http://perso.ens-lyon.fr/christophe.alias/impact2011/impact-07.pdf},
}

@inproceedings{Bondhugula2010xlc,
 author    = {Bondhugula, Uday and
              G{\"u}nl{\"u}k, Oktay and
              Dash, Sanjeeb and
              Renganarayanan, Lakshminarayanan},
 title     = {A model for fusion and code motion in an automatic parallelizing
              compiler},
 booktitle = {International Conference on Parallel Architectures and Compilation Techniques (PACT)},
 year      = {2010},
 pages     = {343-352},
 url = "http://drona.csa.iisc.ernet.in/~uday/publications/uday-pact10.pdf",
 abstract = "
Loop fusion has been studied extensively, but in a manner isolated from other
transformations. This was mainly due to the lack of a powerful intermediate
representation for application of compositions of high-level transformations.
Fusion presents strong interactions with parallelism and locality. Currently,
there exist no models to determine good fusion structures integrated with all
components of an auto-parallelizing compiler. This is also one of the reasons
why all the benefits of optimization and automatic parallelization of long
sequences of loop nests spanning hundreds of lines of code have never been
explored.

We present a fusion model in an integrated automatic parallelization framework
that simultaneously optimizes for hardware prefetch stream buffer utilization,
locality, and parallelism. Characterizing the legal space of fusion structures
in the polyhedral compiler framework is not difficult. However, incorporating
useful optimization criteria into such a legal space to pick good fusion
structures is very hard. The model we propose captures utilization of hardware
prefetch streams, loss of parallelism, as well as constraints imposed by
privatization and code expansion into a single convex optimization space. The
model scales very well to program sections spanning hundreds of lines of code.
It has been implemented into the polyhedral pass of the IBM XL optimizing
compiler. Experimental results demonstrate its effectiveness in finding good
fusion structures for codes including SPEC benchmarks and large applications.
An improvement ranging from 5% to nearly a factor of 2.75x is obtained over the
current production compiler optimizer on these benchmarks.
"
}

@InProceedings{Trifinovic2010,
 author = {Trifunovi\'c, Konrad and Cohen, Albert and Edelsohn, David
           and Li, Feng and Grosser, Tobias and Jagasia, Harsha
           and Ladelsky, Razya and Pop, Sebastian and Sj\"odin, Jan
           and Upadrasta, Ramakrishna },
 title = {{GRAPHITE} Two Years After: First Lessons Learned From
           eal-World Polyhedral Compilation},
 booktitle = {2nd GCC Research Opportunities Workshop (GROW)},
 year = 2010,
 url = "http://citeseerx.ist.psu.edu/viewdoc/download?rep=rep1&type=pdf&doi=10.1.1.220.3386",
 abstract = "
Modern compilers are responsible for adapting the semantics of source programs
into a form that makes efficient use of a highly complex, heterogeneous
machine. This adaptation amounts to solve an optimization problem in a huge and
unstructured search space, while predicting the performance outcome of complex
sequences of program transformations. The polyhedral model of compilation is
aimed at these challenges. Its geometrical, non-inductive semantics enables the
construction of better-structured optimization problems and precise analytical
models. Recent work demonstrated the scalability of the main polyhedral
algorithms to real-world programs. Its integration into production compilers is
under way, pioneered by the Graphite branch of the GNU Compiler Collection
(GCC). Two years after the effective beginning of the project, this paper
reports on original questions and innovative solutions that arose during the
design and implementation of Graphite.
"
}

@inproceedings{Verdoolaege2010isl,
 author = {Verdoolaege, Sven},
 title = {isl: {A}n Integer Set Library for the Polyhedral Model},
 booktitle = {Mathematical Software (ICMS 2010)},
 year = {2010},
 pages={299--302},
 series={LNCS 4151},
 editor={Andres Iglesias and Nobuki Takayama},
 publisher = {Springer-Verlag},
}

@inproceedings{Benabderrahmane2010polyhedral,
  title={The polyhedral model is more widely applicable than you think},
  author={Benabderrahmane, Mohamed-Walid and Pouchet, Louis-No{\"e}l and Cohen, Albert and Bastoul, C{\'e}dric},
  booktitle={Compiler Construction},
  pages={283--303},
  year={2010},
  organization={Springer},
  abstract={
The polyhedral model is a powerful framework for automatic optimization and
parallelization. It is based on an algebraic representation of programs,
allowing to construct and search for complex sequences of optimizations. This
model is now mature and reaches production compilers. The main limitation of
the polyhedral model is known to be its restriction to statically predictable,
loop-based program parts. This paper removes this limitation, allowing to
operate on general data-dependent control-flow. We embed control and exit
predicates as first-class citizens of the algebraic representation, from
program analysis to code generation. Complementing previous (partial) attempts
in this direction, our work concentrates on extending the code generation step
and does not compromise the expressiveness of the model. We present
experimental evidence that our extension is relevant for program optimization
and parallelization, showing performance improvements on benchmarks that were
thought to be out of reach of the polyhedral model.
},
  pdf={http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.162.407&rep=rep1&type=pdf}
}

@inproceedings{Reservoir2010,
 author = {Leung, Allen and Vasilache, Nicolas and Meister, Beno\^{\i}t and
Baskaran, Muthu and Wohlford, David and Bastoul, C{\'e}dric and Lethin,
Richard},
 title = {A mapping path for multi-{GPGPU} accelerated computers from a
portable high level programming abstraction},
 booktitle = {3rd Workshop on General-Purpose Computation on Graphics
Processing Units (GPGPU)},
 series = {GPGPU '10},
 year = {2010},
 publisher = {ACM},
 address = {New York, NY, USA},
 url = "http://www.researchgate.net/publication/220938996_A_mapping_path_for_multi-GPGPU_accelerated_computers_from_a_portable_high_level_programming_abstraction/file/79e41502512b8a3f2b.pdf",
 abstract = "
Programmers for GPGPU face rapidly changing substrate of programming
abstractions, execution models, and hardware implementations. It has been
established, through numerous demonstrations for particular conjunctions of
application kernel, programming languages, and GPU hardware instance, that it
is possible to achieve significant improvements in the price/performance and
energy/performance over general purpose processors. But these demonstrations
are each the result of significant dedicated programmer labor, which is likely
to be duplicated for each new GPU hardware architecture to achieve performance
portability.

This paper discusses the implementation, in the R-Stream compiler, of a source
to source mapping pathway from a high-level, textbook-style algorithm
expression method in ANSI C, to multi-GPGPU accelerated computers. The compiler
performs hierarchical decomposition and parallelization of the algorithm
between and across host, multiple GPGPUs, and within-GPU. The semantic
transformations are expressed within the polyhedral model, including
optimization of integrated parallelization, locality, and contiguity tradeoffs.
Hierarchical tiling is performed. Communication and synchronizations operations
at multiple levels are generated automatically. The resulting mapping is
currently emitted in the CUDA programming language.

The GPU backend adds to the range of hardware and accelerator targets for
R-Stream and indicates the potential for performance portability of single
sources across multiple hardware targets.
"
}

@inproceedings{Sjodin2009design,
 title = {Design of graphite and the polyhedral compilation package},
 author = {Sj{\"o}din, Jan and Pop, Sebastian and Jagasia, Harsha and Grosser, Tobias and Pop, Antoniu and others},
 booktitle = {GCC Developers' Summit},
 year = {2009},
 abstract = "
Graphite is the loop transformation framework that was introduced in GCC 4.4.
This paper gives a detailed description of the design and future directions of
this infrastructure. Graphite uses the polyhedral model as the internal
representation (GPOLY). The plan is to create a polyhedral compilation package
(PCP) that will provide loop optimization and analysis capabilities to GCC.
This package will be separated from GIMPLE via an interface language that is
restricted to express only what GPOLY can represent. The interface language is
a set of data structures that encodes the control flow and memory accesses of a
code region. A syntax for the language is also defined to facilitate debugging
and testing.",
 url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.158.2112&rep=rep1&type=pdf},
}

@article{Baskaran2009,
 author = {Baskaran, Muthu Manikandan and Vydyanathan, Nagavijayalakshmi and
           Bondhugula, Uday Kumar Reddy and Ramanujam, J. and Rountev, Atanas
           and Sadayappan, P.},
 title = {Compiler-assisted dynamic scheduling for effective parallelization of
          loop nests on multicore processors},
 journal = {SIGPLAN Notices},
 issue_date = {April 2009},
 volume = {44},
 number = {4},
 month = feb,
 year = {2009},
 issn = {0362-1340},
 pages = {219--228},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1594835.1504209},
 doi = {10.1145/1594835.1504209},
 acmid = {1504209},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compile-time optimization, dynamic scheduling, run-time
             optimization},
 abstract = {
  Recent advances in polyhedral compilation technology have made it feasible to
  automatically transform affine sequential loop nests for tiled parallel
  execution on multi-core processors. However, for multi-statement input
  programs with statements of different dimensionalities, such as Cholesky or
  LU decomposition, the parallel tiled code generated by existing automatic
  parallelization approaches may suffer from significant load imbalance,
  resulting in poor scalability on multi-core systems. In this paper, we develop
  a completely automatic parallelization approach for transforming input affine
  sequential codes into efficient parallel codes that can be executed on a
  multi-core system in a load-balanced manner. In our approach, we employ a
  compile-time technique that enables dynamic extraction of inter-tile
  dependences at run-time, and dynamic scheduling of the parallel tiles on the
  processor cores for improved scalable execution. Our approach obviates the
  need for programmer intervention and re-writing of existing algorithms for
  efficient parallel execution on multi-cores. We demonstrate the usefulness of
  our approach through comparisons using linear algebra computations: LU and
  Cholesky decomposition.
 }
}

@inproceedings{Verdoolaege2009equivalence,
 author = "Verdoolaege, Sven and Janssens, Gerda and Bruynooghe, Maurice",
 title = "Equivalence checking of static affine programs using widening to handle recurrences",
 booktitle = "Computer Aided Verification 21",
 month = Jun,
 year = 2009,
 pages = "599--613",
 publisher = "Springer",
 url = "http://www.cs.kuleuven.ac.be/publicaties/rapporten/cw/CW565.pdf",
 abstract = "
Designers often apply manual or semi-automatic loop and data transformations on
array and loop intensive programs to improve performance. The transformations
should preserve the functionality, however, and this paper presents an
automatic method for constructing equivalence proofs for the class of static
affine programs. The equivalence checking is performed on a dependence graph
abstraction and uses a new approach based on widening to handle recurrences.
Unlike transitive closure based approaches, this widening approach can also
handle non-uniform recurrences. The implementation is publicly available and is
the first of its kind to fully support commutative operations.
"
}

@article{Bondhugula2008Pluto,
 author = {Bondhugula, Uday and Hartono, Albert and Ramanujam, J. and Sadayappan, P.},
 title = {A practical automatic polyhedral parallelizer and locality optimizer},
 journal = {SIGPLAN Notices},
 volume = {43},
 number = {6},
 year = {2008},
 issn = {0362-1340},
 pages = {101--113},
 doi = {http://doi.acm.org/10.1145/1379022.1375595},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Bondhugula08cc,
    author = {Uday Bondhugula and Muthu Baskaran and Sriram
        Krishnamoorthy and J. Ramanujam and A. Rountev and P.
            Sadayappan},
    title = {Automatic Transformations for Communication-Minimized
        Parallelization and Locality Optimization in the Polyhedral Model},
    booktitle = {International Conference on Compiler Construction
        (ETAPS CC)},
    year = 2008,
    month = apr,
    url = {http://drona.csa.iisc.ernet.in/~uday/publications/uday-cc08.pdf},
    abstract = {

    The polyhedral model provides powerful abstractions to optimize loop nests with 
    regular accesses.  Affine transformations in this model capture a complex
    sequence of execution-reordering loop transformations that can
    improve performance by parallelization as well as locality
    enhancement.  Although a significant body of research has addressed
    affine scheduling and partitioning, the problem of automatically
    finding good affine transforms for communication-optimized
    coarse-grained parallelization together with locality optimization
    for the general case of arbitrarily-nested loop sequences remains a
    challenging problem.

    We propose an automatic transformation framework to
    optimize arbitrarily-nested loop sequences with affine dependences
    for parallelism and locality simultaneously. The approach finds
    good tiling hyperplanes by embedding a powerful and versatile cost
    function into an Integer Linear Programming formulation. These
    tiling hyperplanes are used for communication-minimized
    coarse-grained parallelization as well as for locality
    optimization. The approach enables the minimization of inter-tile
    communication volume in the processor space, and minimization of
    reuse distances for local execution at each node. Programs
    requiring one-dimensional versus multi-dimensional time schedules
    (with scheduling-based approaches) are all handled with the same
    algorithm.  Synchronization-free parallelism, permutable loops or
    pipelined parallelism at various levels can be detected.
    Preliminary studies of the framework show promising results.
    }
}

@TECHREPORT{Chen2008chill,
 author = {Chen, Chun and Chame, Jacqueline and Hall, Mary},
 title = {A framework for composing high-level loop transformations},
 institution = {USC},
 year = {2008},
 url = "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.214.8396&rep=rep1&type=pdf",
 abstract = "
This paper describes a general and robust loop transformation framework that
enables compilers to generate efficient code on complex loop nests. Despite two
decades of prior research on loop optimization, performance of
compiler-generated code often falls short of manually optimized versions, even
for some well-studied BLAS kernels. There are two primary reasons for this.
First, today’s compilers employ fixed transformation strategies, making it
difficult to adapt to different optimization requirements for different
application codes. Second, code transformations are treated in isolation, not
taking into account the interactions between different transformations. This
paper addresses such limitations in a unified framework that supports a broad
collection of transformations, (permutation, tiling, unroll-and-jam, data
copying, iteration space splitting, fusion, distribution and others), which go
beyond existing polyhedral transformation models. This framework is a key
element of a compiler we are developing which performs empirical optimization
to evaluate a collection of alternative optimized variants of a code segment. A
script interface to code generation and empirical search permits transformation
parameters to be adjusted independently and tested; alternative scripts are
used to represent different code variants. By applying this framework to
example codes, we show performance results on automaticallygenerated code for
the Pentium M and MIPS R10000 that are comparable to the best hand-tuned codes,
and significantly better (up to a 14x speedup) than the native compilers.
"
}

@inproceedings{Vasilache2006real,
 author = {Nicolas Vasilache and
           C{\'e}dric Bastoul and
           Albert Cohen},
 title = {Polyhedral Code Generation in the Real World},
 year = {2006},
 pages = {185-201},
 booktitle = {International Conference on Compiler Construction (CC)},
 address = {Vienna},
 publisher = {Springer},
 series = {LNCS},
 volume = {3923},
 url = "http://icps.u-strasbg.fr/~bastoul/research/papers/VBC06-CC.pdf",
 abstract = "
The polyhedral model is known to be a powerful framework to reason about high
level loop transformations. Recent developments in optimizing compilers broke
some generally accepted ideas about the limitations of this model. First,
thanks to advances in dependence analysis for irregular access patterns, its
applicability which was supposed to be limited to very simple loop nests has
been extended to wide code regions. Then, new algorithms made it possible to
compute the target code for hundreds of statements while this code generation
step was expected not to be scalable. Such theoretical advances and new
software tools allowed actors from both academia and industry to study more
complex and realistic cases. Unfortunately, despite strong optimization
potential of a given transformation for e.g., parallelism or data locality,
code generation may still be challenging or result in high control overhead.
This paper presents scalable code generation methods that make possible the
application of increasingly complex program transformations. By studying the
transformations themselves, we show how it is possible to benefit from their
properties to dramatically improve both code generation quality and space/time
complexity, with respect to the best state-of-the-art code generation tool. In
addition, we build on these improvements to present a new algorithm improving
generated code performance for strided domains and reindexed schedules.
"
}

@article{Girbal2006,
 author = {Girbal, Sylvain and Vasilache, Nicolas and Bastoul, C{\'e}dric and Cohen, Albert and Parello, David and Sigler, Marc and Temam, Olivier},
 title = {Semi-automatic composition of loop transformations for deep parallelism and memory hierarchies},
 journal = {Int. J. Parallel Program.},
 issue_date = {June 2006},
 volume = {34},
 number = {3},
 month = jun,
 year = {2006},
 issn = {0885-7458},
 pages = {261--317},
 numpages = {57},
 doi = {10.1007/s10766-006-0012-3},
 acmid = {1165153},
 publisher = {Kluwer Academic Publishers},
 address = {Norwell, MA, USA},
 keywords = {automatic parallelization, compiler optimization, polyhedral model, semi-automatic program transformation},
}

@PhdThesis{Bastoul2004PhD,
 author = {Bastoul, C\'{e}dric},
 title = {Improving Data Locality in Static Control Programs},
 school = {University Paris 6, Pierre et Marie Curie, France},
 month = dec,
 year = 2004,
 abstract = {
  Cache memories were invented to decouple fast processors from slow memories.
  However, this decoupling is only partial and many researchers have attempted to
  improve cache use by program optimization. Potential benefits are significant
  since both energy dissipation and performance highly depend on the traffic
  between memory levels.
  This thesis will visit most of the steps of high level transformation frameworks
  in the polyhedral model in order to improve both applicability and target code
  quality. To achieve this goal, we refine the concept of static control parts
  (SCoP) and we show experimentally that this program model is relevant to
  real-life applications. We present a transformation policy freed of classical
  limitations like considering only unimodular or invertible functions. Lastly,
  we extend the Quiller\'{e} et al. algorithm to be able to generate efficient codes
  in a reasonable amount of time. To exploit this transformation framework, we
  propose a data locality improvement method based on a singular execution scheme
  where an asymptotic evaluation of the memory traffic is possible. This information
  is used in our optimization algorithm to find the best reordering of the program
  operations, at least in an asymptotic sense. This method considers legality
  and each type of data locality as constraints whose solution is an appropriate
  transformation. The optimizer has been prototyped and tested with non-trivial
  programs. Experimental evidence shows that our framework can improve data
  locality and performance in traditionally challenging programs with e.g.
  non-perfectly nested loops, complex dependences and non-uniformly generated
  references.
 }
}

@InProceedings{Bastoul2004CLooG,
 author = {Bastoul, C\'{e}dric},
 title = {Code Generation in the Polyhedral Model Is Easier Than You Think},
 booktitle = {IEEE International Conference on Parallel Architecture
              and Compilation Techniques},
 year = 2004,
 pages = {7--16},
 month = {September},
 address = {Juan-les-Pins, France},
 abstract = {
  Many advances in automatic parallelization and optimization have been
  achieved through the polyhedral model. It has been extensively shown that this
  computational model provides convenient abstractions to reason about and apply
  program transformations. Nevertheless, the complexity of code generation has
  long been a deterrent for using polyhedral representation in optimizing
  compilers. First, code generators have a hard time coping with generated code
  size and control overhead that may spoil theoretical benefits achieved by the
  transformations. Second, this step is usually time consuming, hampering the
  integration of the polyhedral framework in production compilers or
  feedback-directed, iterative optimization schemes. Moreover, current code
  generation algorithms only cover a restrictive set of possible transformation
  functions. This paper discusses a general transformation framework able to
  deal with non-unimodular, non-invertible, non-integral or even non-uniform
  functions. It presents several improvements to a state-of-the-art code
  generation algorithm. Two directions are explored: generated code size and
  code generator efficiency. Experimental evidence proves the ability of the
  improved method to handle real-life problems.
 }
}

@inproceedings{Darte2001,
 athor= {Alain Darte and Yves Robert and Fr\'ed\'eric Vivien},
 booktitle = {Compiler Optimizations for Scalable Parallel Systems: Languages, Compilation Techniques and Run Time Systems},
 publisher = {Springer Verlag},
 series = {LNCS},
 volume = {1808},
 title = {Loop parallelization algorithms},
 pages = {141-171},
 year = {2001},
 url = "http://graal.ens-lyon.fr/~fvivien/Publications/Chapter-LNCS.pdf",
 abstract = "
This chapter is devoted to a comparative survey of loop parallelization
algorithms. Various algorithms have been presented in the literature, such as
those introduced by Allen and Kennedy, Wolf and Lam, Darte and Vivien, and
Feautrier. These algorithms make use of different mathematical tools. Also,
they do not rely on the same representation of data dependences. In this
chapter, we survey each of these algorithms, and we assess their power and
limitations, both through examples and by stating “optimality” results. An
important contribution of this chapter is to characterize which algorithm is
the most suitable for a given representation of dependences. This result is of
practical interest, as it provides guidance for a compiler-parallelizer: given
the dependence analysis that is available, the simplest and cheapest
parallelization algorithm that remains optimal should be selected.
"
}

@article{Quillere2000,
 author = {Quiller\'{e}, Fabien and Rajopadhye, Sanjay and Wilde, Doran},
 title = {Generation of Efficient Nested Loops from Polyhedra},
 journal = {International Journal of Parallel Programming},
 volume = 28,
 number=5,
 month=oct,
 year = 2000,
 pages={469--498},
 abstract = {
  Automatic parallelization in the polyhedral model is based on affine
  transformations from an original computation domain (iteration space) to a
  target space-time domain, often with a different transformation for each
  variable. Code generation is an often ignored step in this process that has a
  significant impact on the quality of the final code. It involves making a
  trade-off between code size and control code simplification/optimization.
  Previous methods of doing code generation are based on loop splitting,
  however they have non-optimal behavior when working on parameterized
  programs. We present a general parameterized method for code generation based
  on dual representation of polyhedra. Our algorithm uses a simple recursion on
  the dimensions of the domains, and enables fine control over the tradeoff
  between code size and control overhead.
 }
}

@article{Loechner1997parameterized,
  title={Parameterized polyhedra and their vertices},
  author={Loechner, Vincent and Wilde, Doran K},
  journal={International Journal of Parallel Programming},
  volume={25},
  number={6},
  pages={525--549},
  year={1997},
  publisher={Springer},
  abstract={
Algorithms specified for parametrically sized problems are more general
purpose and more reusable than algorithms for fixed sized problems. For this
reason, there is a need for representing and symbolically analyzing linearly
parameterized algorithms. An important class of parallel algorithms can be
described as systems of parameterized affine recurrence equations (PARE). In
this representation, linearly parameterized polyhedra are used to describe the
domains of variables. This paper describes an algorithm which computes the set
of parameterized vertices of a polyhedron, given its representation as a
system of parameterized inequalities. This provides an important tool for the
symbolic analysis of the parameterized domains used to define variables and
computation domains in PAREs. A library of operations on parameterized
polyhedra based on the Polyhedral Library has been written in C and is freely
distributed. 
},
  url={http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.2146&rep=rep1&type=pdf}
}

@article{Kelly1996omega,
 title = {The Omega calculator and library, version 1.1.0},
 author = {Kelly, Wayne and Maslov, Vadim and Pugh, William and Rosser, Evan and Shpeisman, Tatiana and Wonnacott, Dave},
 year = 1996,
 url={http://www.cs.utah.edu/~mhall/cs6963s09/lectures/omega.ps}
}

@inproceedings{Kelly1995code,
 author = "Kelly, Wayne and Pugh, William and Rosser, Evan",
 title = "Code Generation for Multiple Mappings",
 booktitle = "The 5th Symposium on the Frontiers of Massively Parallel Computation",
 address = "McLean, VA",
 year = 1995,
 url = "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.8696&rep=rep1&type=pdf",
 abstract = "
There has been a great amount of recent work toward unifying iteration
reordering transformations. Many of these approaches represent transformations
as affine mappings from the original iteration space to a new iteration space.
These approaches show a great deal of promise, but they all rely on the ability
to generate code that iterates over the points in these new iteration spaces in
the appropriate order. This problem has been fairly well-studied in the case
where all statements use the same mapping. We have developed an algorithm for
the less well-studied case where each statement uses a potentially different
mapping. Unlike many other approaches, our algorithm can also generate code
from mappings corresponding to loop blocking. We address the important
trade-off between reducing control overhead and duplicating code.
"
}

@conference{Kelly1995unifying,
 author = {Kelly, Wayne and Pugh, William},
 title = {A unifying framework for iteration reordering transformations},
 booktitle = {IEEE First Int. Conf. on Algorithms and Architectures for Parallel Processing (ICAPP 95)},
 volume = {1},
 year = {1995},
 month = apr,
 url = "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.1382&rep=rep1&type=ps",
 abstract = "
We present a framework for unifying iteration reordering transformations such
as loop interchange, loop distribution, skewing, tiling, index set splitting
and statement reordering. The framework is based on the idea that a
transformation can be represented as a mapping from the original iteration
space to a new iteration space. The framework is designed to provide a uniform
way to represent and reason about transformations. We also provide algorithms
to test the legality of mappings, and to generate optimized code for mappings.
"
}

@article{Pugh1994static,
  title={Static analysis of upper and lower bounds on dependences and parallelism},
  author={Pugh, William and Wonnacott, David},
  journal={ACM Transactions on Programming Languages and Systems (TOPLAS)},
  volume={16},
  number={4},
  pages={1248--1278},
  year={1994},
  publisher={ACM},
  abstract = {
Existing compilers often fail to parallelize sequential code, even when a
program can be manually transformed into parallel form by a sequence of
well-understood transformations (as in the case for many of the Perfect Club
Benchmark programs). These failures can occur for several reasons: the code
transformations implemented in the compiler may not be sufficient to produce
parallel code, the compiler may not find the proper sequence of
transformations, or the compiler may not be able to prove that one of the
necessary transformations is legal. When a compiler fails to extract sufficient
parallelism from a program, the programmer may try to extract additional
parallelism. Unfortunately, the programmer is typically left to search for
parallelism without significant assistance. The compiler generally does not
give feedback about which parts of the program might contain additional
parallelism, or about the types of transformations that might be needed to
realize this parallelism. Standard program transformations and dependence
abstractions cannot be used to provide this feedback. In this paper, we propose
a two-step approach to the search for parallelism in sequential programs. In
the first step, we construct several sets of constraints that describe, for
each statement, which iterations of that statement can be executed
concurrently. By constructing constraints that correspond to different
assumptions about which dependences might be eliminated through additional
analysis, transformations, and user assertions, we can determine whether we can
expose parallelism by eliminating dependences. In the second step of our search
for parallelism, we examine these constraint sets to identify the kinds of
transformations needed to exploit scalable parallelism. Our tests will identify
conditional parallelism and parallelism that can be exposed by combinations of
transformations that reorder the iteration space (such as loop interchange and
loop peeling). This approach lets us distinguish inherently sequential code
from code that contains unexploited parallelism. It also produces information
about the kinds of transformations needed to parallelize the code, without
worrying about the order of application of the transformations. Furthermore,
when our dependence test is inexact we can identify which unresolved
dependences inhibit parallelism by comparing the effects of assuming dependence
or independence. We are currently exploring the use of this information in
programmer-assisted parallelization.
},
  url={http://drum.lib.umd.edu/bitstream/1903/629/4/CS-TR-3250.pdf},
}

@book{pugh1994exact,
  title={An exact method for analysis of value-based array data dependences},
  author={Pugh, William and Wonnacott, David},
  year={1994},
  publisher={Springer},
  abstract={
Standard array data dependence testing algorithms give information
about the aliasing of array references. If statement 1 writes a[5],
and statement 2 later reads a [5], standard techniques described this
as a flow dependence, even if there was an intervening write. We call
a dependence between two references to the same memory location a
memory-based dependence. In contrast, if there are no intervening
writes, the references touch the same value and we call the
dependence a value-based dependence.

There has been a surge of recent work on value-based array data
dependence analysis (also referred to as computation of array
data-flow dependence information). In this paper, we describe a
technique that is exact over programs without control flow (other
than loops) and non-linear references. We compare our proposal with
the technique proposed by Paul Feautrier, which is the other
technique that is complete over the same domain as ours. We also
compare our work with that of Tu and Padua, a representative
approximate scheme for array privatization.
},
  url={http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.22.8688&rep=rep1&type=pdf},
}

@article{Feautrier1992multi,
 author = {Feautrier, Paul},
 affiliation = {Laboratoire MASI Université de Versailles St-Quentin 45 Avenue
                des Etats-Unis 78035 Versailles Cedex France},
 title = {Some efficient solutions to the affine scheduling problem. Part II.
          Multidimensional time},
 journal = {International Journal of Parallel Programming},
 publisher = {Springer Netherlands},
 issn = {0885-7458},
 keyword = {Informatique},
 pages = {389-420},
 volume = {21},
 issue = {6},
 url = {http://dx.doi.org/10.1007/BF01379404},
 note = {10.1007/BF01379404},
 year = {1992},
 abstract = {
  This paper extends the algorithms which were given in Part I to cases in which
  there is no affine schedule, i.e. to problems whose parallel complexity is
  polynomial but not linear. The natural generalization is to multidimensional
  schedules with lexicographic ordering as temporal succession. Multidimensional
  affine schedules, are, in a sense, equivalent to polynomial schedules, and are
  much easier to handle automatically. Furthermore, there is a strong connexion
  between multidimensional schedules and loop nests, which allows one to prove
  that a static control program always has a multidimensional schedule. Roughly,
  a larger dimension indicates less parallelism. In the algorithm which is
  presented here, this dimension is computed dynamically, and is just sufficient
  for scheduling the source program. The algorithm lends itself to a "divide and
  conquer" strategy. The paper gives some experimental evidence for the
  applicability, performances and limitations of the algorithm.
 }
}

@article{Feautrier1992one,
 author = {Feautrier, Paul},
 title = {Some efficient solutions to the affine scheduling problem: Part I.
          One-dimensional time},
 journal = {International Journal of Parallel Programming},
 issue_date = {Oct. 1992},
 volume = {21},
 number = {5},
 month = oct,
 year = {1992},
 issn = {0885-7458},
 pages = {313--348},
 numpages = {36},
 url = {http://dx.doi.org/10.1007/BF01407835},
 doi = {10.1007/BF01407835},
 acmid = {171448},
 publisher = {Kluwer Academic Publishers},
 address = {Norwell, MA, USA},
 keywords = {automatic parallelization, automatic systolic array design,
             scheduling},
 abstract = {
  Programs and systems of recurrence equations may be represented as
  sets of actions which are to be executed subject to precedence constraints. In
  many cases, actions may be labelled by integral vectors in some iteration
  domain, and precedence constraints may be described by affine relations. A
  schedule for such a program is a function which allows one to estimate the
  intrinsic degree of parallelism of the program and to compile a parallel
  version for multiprocessor architectures or systolic arrays. This paper deals
  with the problem of finding closed form schedules as affine or piecewise
  affine functions of the iteration vector. An efficient algorithm which
  reduces the scheduling problem to a parametric linear program of small size,
  which can be readily solved by an efficient algorithm.
 }
}

@article{Feautrier1991,
 title = {Dataflow analysis of array and scalar references},
 author = {Feautrier, Paul},
 journal = {International Journal of Parallel Programming},
 volume = {20},
 number = {1},
 pages = {23--53},
 year = {1991},
 publisher = {Springer},
 abstract = {
  Given a program written in a simple imperative language (assignment
  statements,for loops, affine indices and loop limits), this paper presents an
  algorithm for analyzing the patterns along which values flow as the execution
  proceeds. For each array or scalar reference, the result is the name and
  iteration vector of the source statement as a function of the iteration
  vector of the referencing statement. The paper discusses several applications
  of the method: conversion of a program to a set of recurrence equations,
  array and scalar expansion, program verification and parallel program
  construction.
 }
}

@inproceedings{Ancourt1991scanning,
  title={Scanning polyhedra with DO loops},
  author={Ancourt, Corinne and Irigoin, Fran{\c{c}}ois},
  booktitle={ACM Sigplan Notices},
  volume={26},
  number={7},
  pages={39--50},
  year={1991},
  organization={ACM},
  abstract={
Supercompilers perform complex program transformations which often result in
new loop bounds. This paper shows that, under the usual assumptions in
automatic parallelization, most transformations on loop nests can be expressed
as affine transformations on integer sets defined by polyhedra and that the new
loop bounds can be computed with algorithms using Fourier's pairwise
elimination method although it is not exact for integer sets. Sufficient
conditions to use pairwise elimination on integer sets and to extend it to
pseudo-linear constraints are also given. A tradeoff has to be made between
dynamic overhead due to some bound slackness and compilation complexity but the
resulting code is always correct. Thse algorithms can be used to onterchange or
block loops regardless of the loop bounds or the blocking strategy and to
safety exchange array parts between two levels of a memory hierarchy or between
neighboring processors in a distributed memory machine.
},
  url={http://hal.inria.fr/docs/00/75/27/74/PDF/A-195.pdf}
}

@article{Feautrier1988parametric,
  title={Parametric integer programming},
  author={Feautrier, Paul},
  journal={RAIRO Recherche op{\'e}rationnelle},
  volume={22},
  number={3},
  pages={243--268},
  year={1988},
  abstract={
When analysising computer programs (especially numercial programs in which
arrays are used extensively), one is often confronted with integer programing
problems. These problems have three peculiarities:
  1) feasible points are ranked according to lexicographic order rathern than
the usual linear economic function;
  2) the feasible set depends on integer parameters;
  3) one is interested only in exact solutions.

The difficulty is somewhat alleviated by the fact that problems sizes are usually quite small. In this paper
we show thath:
  1) the classical simplex algorithm has no difficulty in handling lexicographic ordering;
  2) the algorithm may be executed in symbolic mode, this giving the solution of continuous parametric problems;
  3) the method may be extended to problems in integers.

We prove that the resulting algorithm always terminate and give an estimative of its complexity.
},
  url={http://camlunity.ru/swap/Library/Conflux/Techniques%20-%20Code%20Analysis%20and%20Transformations%20%28Polyhedral%29/Integer%20Programming/parametric_integer_programming.pdf},
}

@inproceedings{feautrier1988array,
  title={Array expansion},
  author={Feautrier, Paul},
  booktitle={Proceedings of the 2nd international conference on Supercomputing},
  pages={429--441},
  year={1988},
  organization={ACM},
  abstract={
A common problem in restructuring programs for vector or parallel execution is
the suppression of false dependencies which originate in the reuse of the same
memory cell for unrelated values. The method is simple and well understood in
the case of scalars. This paper gives the general solution for the case of
arrays. The expansion is done in two steps: first, modify all definitions of
the offending array in order to obtain the single assignment property. Then,
reconstruct the original data flow by adapting all uses of the array. This is
done with the help of a new algorithm for solving parametric integer programs.
The technique is quite general and may be used for other purposes, including
program checking, collecting array predicates, etc ...
},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.29.5704&rep=rep1&type=pdf}
}

@article{Lamport1974,
 title = {The parallel execution of DO loops},
 author = {Lamport, Leslie},
 journal = {Communications of the ACM},
 volume = {17},
 number = {2},
 pages = {83--93},
 year = {1974},
 publisher = {ACM},
 url = {http://research.microsoft.com/en-us/um/people/lamport/pubs/do-loops.pdf},
 abstract = {
  Methods are developed for the parallel execution of different iterations of a
  DO loop. Both asynchronous multiprocessor computers and array computers are
  considered. Practical application to the design of compilers for such
  computers is discussed.
 }
}

@article{Karp1967,
 title = {The organization of computations for uniform recurrence equations},
 author = {Karp, Richard M and Miller, Raymond E and Winograd, Shmuel},
 journal = {Journal of the ACM},
 volume = {14},
 number = {3},
 pages = {563--590},
 year = {1967},
 publisher = {ACM},
 url = {http://dl.acm.org/citation.cfm?id=321418},
 abstract = {
  A set equations in the quantities ai(p), where i = 1, 2, · · ·, m and p ranges
  over a set R of lattice points in n-space, is called a system of uniform
  recurrence equations if the following property holds: If p and q are in R and
  w is an integer n-vector, then ai(p) depends directly on aj(p - w) if and
  only if ai(q) depends directly on aj(q - w). Finite-difference approximations
  to systems of partial differential equations typically lead to such recurrence
  equations. The structure of such a system is specified by a dependence graph G
  having m vertices, in which the directed edges are labeled with integer
  n-vectors. For certain choices of the set R, necessary and sufficient
  conditions on G are given for the existence of a schedule to compute all the
  quantities ai(p) explicitly from their defining equations. Properties of such
  schedules, such as the degree to which computation can proceed “in parallel,”
  are characterized. These characterizations depend on a certain iterative
  decomposition of a dependence graph into subgraphs. Analogous results
  concerning implicit schedules are also given.
 }
}
