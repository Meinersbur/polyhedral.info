@Comment{{
We try to keep the formatting in this bibtex file consistent. Please
try to follow the following style guide.

- Order: The entries of this file are ordered by year of appearance and
          then by the bibtex tags (newest entries at the top).
- Keys:  Use the style firstauthor.lastname + year + optional-tag.
         E.g. [Feautrier1992multi]
- '{}': Use a single pair of braces and embrace individual words/letters
         that should always remain uppercase.
- Abbreviations: Do not abbreviate conferences and journal names.
- Abstracts: Include abstracts, if available.
- ACM style: For all remaining style issues, we to follow the style used by ACM
         (see e.g., Baskaran2009)

!! The style rules are necessarily incomplete, if you would like to improve
   the style of this file, feel free to provide a patch that both extends
   the style guide and fixes the existing entries.
}}

@inproceedings{Juega2014cgo,
 author = {Juega, Carlos and P\'{e}rez, Jos\'{e} Ignacio G\'{o}mez and Tenllado, Christian and Catthoor, Francky Catthoor},
 title = {Adaptive Mapping and Parameter Selection Scheme to Improve Automatic
Code Generation for GPUs},
 booktitle = {{Intl. Symp. on Code Generation and Optimization (CGO)}},
 year = 2014,
 address = {Orlando, FL, United States},
}

@inproceedings{Grosser2014cgo,
 author = {Grosser, Tobias and Cohen, Albert and Holewinski, Justin and Sadayappan, P. and Verdoolaege, Sven},
 title = {{Hybrid Hexagonal/Classical Tiling for GPUs}},
 booktitle = {{Intl. Symp. on Code Generation and Optimization (CGO)}},
 year = 2014,
 address = {Orlando, FL, United States},
 url = {http://hal.inria.fr/hal-00911177}
}

@inproceedings{Jimborean2014cgo,
 author = {Jimborean, Alexandra and Koukos, Konstantinos and Spiliopoulos, Vasileios and
 Black-Schaffer, David and Kaxiras, Stefanos},
 title = {{Fix the code. Don't tweak the hardware: A new compiler approach to Voltage-Frequency scaling}},
 booktitle = {{Intl. Symp. on Code Generation and Optimization (CGO)}},
 year = 2014,
 address = {Orlando, FL, United States},
}

@inproceedings{Venkat2014cgo,
 author = {Venkat, Anand and Shantharam, Manu and Hall, Mary and Strout, Michelle},
 title = {{Non-affine Extensions to Polyhedral Code Generation}},
 booktitle = {{Intl. Symp. on Code Generation and Optimization (CGO)}},
 year = 2014,
 address = {Orlando, FL, United States},
}

@inproceedings{Darte2014impact,
 author = {Darte, Alain and Isoard, Alexandre},
 title = {Parametric Tiling with Inter-Tile Data Reuse},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria},
 url = {http://impact.gforge.inria.fr/impact2014/papers/impact2014-darte.pdf},
 abstract = {
  Loop tiling is a loop transformation widely used to improve spatial and
  temporal data locality, increase computation granularity, and enable blocking
  algorithms, which are particularly useful when offloading kernels on
  platforms with small memories. When hardware caches are not available, data
  transfers must be software-managed: they can be reduced by exploiting data
  reuse between tiles and, this way, avoid some useless external communications.
  An important parameter of loop tiling is the sizes of the tiles, which impact
  the size of the necessary local memory. However, for most analyzes that
  involve several tiles, which is the case for intertile data reuse, the tile
  sizes induce non-linear constraints, unless they are numerical constants.
  This complicates or prevents a parametric analysis. In this paper, we show
  that, actually, parametric tiling with inter-tile data reuse is nevertheless
  possible, i.e., it is possible to determine, at compile-time and in a
  parametric fashion, the copy-in and copy-out data sets for all tiles, with
  inter-tile reuse, as well as the sizes of the induced local memories, without
  the need to analyze the code for each tile size.
 }
}
@inproceedings{Guo2014impact,
 author = {Guo, Jing and Bernecky, Robert and
 		Thiyagalingam, Jeyarajan and Scholz, Sven-Bodo},
 title = {Polyhedral Methods for Improving Parallel Update-in-Place},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria},
 url = {http://impact.gforge.inria.fr/impact2014/papers/impact2014-guo.pdf},
 abstract = {
We demonstrate an optimization, denoted as polyhedral reuse analysis (PRA),
that uses polyhedral methods to improve the analysis of in-place update for
single-assignment arrays.  The PRA optimization attempts to determine when
parallel array operations that jointly deffine new arrays from existing ones can
reuse the memory of the existing arrays, rather than creating new ones.
Polyhedral representations and related dependency inference methods facilitate
that analysis.

In the context of SaC , we demonstrate the impact of this
optimisation using two non-trivial benchmarks evaluated on conventional shared
memory machines and on GPUs, obtaining performance improvements of 2-8 times
for LU Decomposition and of 2-10 times for Needleman-Wunsch, over the same
computations with PRA disabled.
 }
}
@inproceedings{Iooss2014impact,
 author = {Iooss, Guillaume and Rajopadhye, Sanjay and
 	Alias, Christophe and Zou, Yun},
 title = {Constant Aspect Ratio Tiling},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria},
 url = {http://impact.gforge.inria.fr/impact2014/papers/impact2014-iooss.pdf},
 abstract = {
Parametric tiling is a well-known transformation which is widely used to improve
locality, parallelism and granularity.  However, parametric tiling is also a
non-linear transformation and this prevents polyhedral analysis or further
polyhedral transformation after parametric tiling. It is therefore generally
applied during the code generation phase.

In this paper, we present a method
to remain polyhedral, in a special case of parametric tiling, where all the
dimensions are tiled and all the tile sizes are constant multiples of a single
tile size parameter. We call this Constant Aspect Ratio Tiling . We show how to
mathematically transform a polyhedron and an affine function into their tiled
counterpart, which are the two main operations needed in such transformation.
}
}
@inproceedings{Li2014impact,
 author = {Li, Peng and Pouchet, Louis-No{\"e}l and Cong, Jason},
 title = {Throughput Optimization for High-Level Synthesis
 		Using Resource Constraints},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria}
}
@inproceedings{Mullapudi2014impact,
 author = {Mullapudi, Ravi Teja and Bondhugula, Uday},
 title = {Tiling for Dynamic Scheduling},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria},
 url = {http://impact.gforge.inria.fr/impact2014/papers/impact2014-mullapudi.pdf},
 abstract = {
Tiling is a key transformation used for coarsening the granularity of
parallelism and improving locality. It is known that current state-of-the-art
compiler approaches for tiling affine loop nests make use of sufficient, i.e.,
conservative conditions for the validity of tiling.  These conservative
conditions, which are used for static scheduling, miss tiling schemes for which
the tile schedule is not easy to describe statically. However, the partial
order of the tiles can be expressed using dependence relations which can be
used for dynamic scheduling at runtime. Another set of opportunities are missed
due to the classic reason that finding valid tiling hyperplanes is often harder
than checking whether a given tiling is valid.

Though the conservative
conditions for validity of tiling have worked in practice on a large number of
codes, we show that they fail to find the desired tiling in several cases –
some of these have dependence patterns similar to real world problems and
applications. We then look at ways to improve current techniques to address
this issue. To quantify the potential of the improved techniques, we manually
tile two dynamic programming algorithms – the Floyd-Warshall algorithm, and
Zuker’s RNA secondary structure prediction and report their performance on a
shared memory multicore. Our 3-d tiled dynamically scheduled implementation of
Zuker’s algorithm outperforms an optimized multi-core implementation GTfold by
a factor of 2.38. Such a 3-d tiling was possible only by reasoning with more
precise validity conditions
}
}
@inproceedings{Simbuerger2014impact,
 author = {Simb{\"u}rger, Andreas and Gr{\"o}{\ss}liger, Armin},
 title = {On the Variety of Static Control Parts in Real-World Programs:
     from Affine via Multi-dimensional to Polynomial and Just-in-Time},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria},
 url = {http://impact.gforge.inria.fr/impact2014/papers/impact2014-simbuerger.pdf},
 abstract = {
The polyhedron model has been used successfully for automatic parallelization
of code regions with loop nests satisfying certain restrictions, so-called
static control parts. A popular implementation of this model is Polly (an
extension of the LLVM compiler), which is able to identify static control parts
in the intermediate representation of the compiler. We look at static control
parts found in 50 real-world programs from different domains. We study whether
these programs are amenable to polyhedral optimization by Polly at compile time
or at run time. We report the number of static control parts with uniform or
authorne dependences found and study extensions of the current implementation
in Polly . We consider extensions which handle multi-dimensional arrays with
parametric sizes and arrays represented by "pointer-to-pointer" constructs. In
addition, we extend the modeling capabilities of Polly to a model using
semi-algebraic sets and real algebra instead of polyhedra and linear algebra.
We do not only consider the number and size of the code regions found but
measure the share of the run time the studied programs spend in the identified
regions for each of the classes of static control parts under study.
}
}
@inproceedings{Verdoolaege2014impact,
 author = {Verdoolaege, Sven and Guelton, Serge and
 	Grosser, Tobias and Cohen, Albert},
 title = {Schedule Trees},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria},
 url = {http://impact.gforge.inria.fr/impact2014/papers/impact2014-verdoolaege.pdf},
 abstract = {
 Schedules in the polyhedral model, both those that represent the original
execution order and those produced by scheduling algorithms, naturally have the
form of a tree. Generic schedule representations proposed in the literature
encode this tree structure such that it is only implicitly available.
Following the internal representation of isl , we propose to represent
schedules as explicit trees and further extend the concept by introducing
different kinds of nodes. We compare our schedule trees to other
representations in detail and illustrate how they have been successfully used
to simplify the implementation of a non-trivial polyhedral compiler.
 }
}
@inproceedings{Wang2014impact,
 author = {Wang, Wei and Cavazos, John and Porterfield, Allan},
 title = {Energy Auto-tuning using the Polyhedral Approach},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria},
 url = {http://impact.gforge.inria.fr/impact2014/papers/impact2014-wang.pdf},
 abstract = {
As the HPC community moves into the exascale computing era, application energy
has become a big concern. Tuning for energy will be essential in the effort to
overcome the limited power envelope. How is tuning for lower energy related to
tuning for faster execution? Understanding that relationship can guide both
performance and energy tuning for exascale.  In this paper, a strong
correlation is presented between the two that allows tuning for execution to be
used as a proxy for energy tuning. We also show that polyhedral compilers can
ectively tune a realistic application for both time and energy.

For a large
number of variants of the Polybench programs and LULESH energy consumption is
strongly correlated with total execution time. Optimizations can increase the
power and energy required between variants, but the variant with minimum
execution time also has the lowest energy usage. The polyhedral framework was
also used to optimize a 2D cardiac wave propagation simulation application.
Various loop optimizations including fusion, tiling, vectorization, and
auto-parallelization, achieved a 20% speedup over the baseline OpenMP
implementation, with an equivalent reduction in energy on an Intel Sandy Bridge
system.  On an Intel Xeon Phi system, improvements as high as 21% in execution
time and 19% reduction in energy are obtained

}
}
@inproceedings{Yuki2014impact,
 author = {Yuki, Tomofumi},
 title = {Understanding {PolyBench/C} 3.2 Kernels},
 booktitle = {Proceedings of the
     4th International Workshop on Polyhedral Compilation Techniques},
 editor = {Rajopadhye, Sanjay and Verdoolaege, Sven},
 year   = 2014,
 month  = Jan,
 address = {Vienna, Austria},
 url = {http://impact.gforge.inria.fr/impact2014/papers/impact2014-yuki.pdf},
 abstract = {
 In this position paper, we argue the need for more rigorous specification of
kernels in the PolyBench/C benchmark suite.  Currently, the benchmarks are
mostly specified by their implementation as C code, with a one sentence
description of what the code is supposed to do. While this is suchcient in the
context of automated loop transformation, the lack of precise specification may
have let some questionable behaviors as benchmark kernels remain unnoticed.

As
an extreme example, two kernels in PolyBench/C 3.2 exhibit parametric speed up
with respect to the problem size when its questionable properties are used.
Abusing such properties can provide arbitrary speedup, which can be some factor
of millions, potentially threatening the credibility of any experimental
evaluation using PolyBench.
 }
}

@ARTICLE{Gonzalez2013tpds,
  author =  "A. Gonzalez-Escribano and Y. Torres and J. Fresno and D. Llanos",
  title =  "An extensible system for multilevel automatic data partition and mapping",
  journal =  "IEEE Transactions on Parallel and Distributed Systems ",
  year =  "2013",
  month =  "March",
  doi="10.1109/TPDS.2013.83",
  abstract = "
Automatic data distribution is a key feature to obtain efficient
implementations from abstract and portable parallel codes. We present a highly
efficient and extensible runtime library that integrates techniques for
automatic data partition and mapping. It uses a novel approach to define an
abstract interface and a plug-in system to encapsulate different types of
regular and irregular techniques, helping to generate codes which are
independent of the exact mapping functions selected. Currently, it supports
hierarchical tiling of arrays with dense and stride domains, that allows the
implementation of both data and task parallelism using a SPMD model. It
automatically computes appropriate domain partitions for a selected virtual
topology, mapping them to available processors with static or dynamic
load-balancing techniques. Our library also allows the construction of reusable
communication patterns that efficiently exploit MPI communication capabilities.
The use of our library greatly reduces the complexity of data distribution and
communication, hiding the details of the underlying architecture. The library
can be used as an abstract layer for building generic tiling operations as
well. Our experimental results show that the use of this library allows to
achieve similar performance as carefully-implemented manual versions for
several, well-known parallel kernels and benchmarks in distributed and
multicore systems, and substantially reduces programming effort.

",
  url = "http://www.computer.org/csdl/trans/td/preprint/06482561-abs.html"
}

@ARTICLE{Fresno2013js,
  author =  "J. Fresno and A. Gonzalez-Escribano and D. Llanos",
  title =  "Extending a Hierarchical Tiling Arrays Library to Support Sparse Data Partitioning",
  journal =  "The Journal of Supercomputing",
  year =  "2013",
  volume =  "64",
  number =  "1",
  pages =  "59--68",
  month =  "April",
  doi = "10.1007/s11227-012-0757-y",
  abstract = "
Layout methods for dense and sparse data are often seen as two separate
problems with their own particular techniques. However, they are based on the
same basic concepts. This paper studies how to integrate automatic data-layout
and partition techniques for both dense and sparse data structures. In
particular, we show how to include support for sparse matrices or graphs in
Hitmap, a library for hierarchical tiling and automatic mapping of arrays. The
paper shows that it is possible to offer a unique interface to work with both
dense and sparse data structures. Thus, the programmer can use a single and
homogeneous programming style, reducing the development effort and simplifying
the use of sparse data structures in parallel computations. Our experimental
evaluation shows that this integration of techniques can be effectively done
without compromising performance.
",
  url = "http://link.springer.com/article/10.1007%2Fs11227-012-0757-y"
}

@INPROCEEDINGS{Torres2013pdpta,
  author =  "Y. Torres and A. Gonzalez-Escribano and D. Llanos",
  title =  "Automatic Run-time Mapping of Polyhedral Computations to Heterogeneous Devices with Memory-size Restrictions",
  booktitle =  "PDPTA'13 - The 2013 International Conference on Parallel and Distributed Processing Techniques and Applications",
  year =  "2013",
  volume =  "2",
  month =  "July",
  publisher =  "CSREA Press",
  isbn = "1-60132-256-9, 1-60132-257-7 (1-60132-258-5)",
  abstract = "
Tools that aim to automatically map parallel computations to heterogeneous and
hierarchical systems try to divide the whole computation in parts with
computational loads adjusted to the capabilities of the target devices. Some
parts are executed in node cores, while others are executed in accelerator
devices.  Each part requires one or more data-structure pieces that should be
allocated in the device memory during the computation.

In this paper we present a model that allows such automatic mapping tools to
transparently assign computations to heterogeneous devices with different
memory size restrictions. The model requires the programmer to specify the
access patterns of the computation threads in a simple abstract form. This
information is used at run-time to determine the second-level partition of the
computation assigned to a device, ensuring that the data pieces required by
each sub-part fit in the target device memory, and that the number of kernels
launched is minimal.  We present experimental results with a prototype
implementation of the model that works for regular polyhedral expressions. We
show how it works for different example applications and access patterns,
transparently executing big computations in devices with different memory size
restrictions.
",
  url = {http://www.infor.uva.es/~diego/docs/torres13.pdf}
}

@techreport{Grosser2013Promises,
 hal_id = {hal-00848691},
 url = {http://hal.inria.fr/hal-00848691},
 title = {{The Promises of Hybrid Hexagonal/Classical Tiling for GPU}},
 author = {Grosser, Tobias and Verdoolaege, Sven and Cohen, Albert and Sadayappan, P.},
 abstract = {
  Time-tiling is necessary for efficient execution of iterative
  stencil computations. But the usual hyper-rectangular tiles cannot
  be used because of positive/negative dependence distances along the
  stencil's spatial dimensions. Several prior efforts have addressed
  this issue. However, known techniques trade enhanced data reuse
  for other causes of inefficiency, such as unbalanced parallelism,
  redundant computations, or increased control flow overhead incompatible
  with efficient GPU execution. We explore a new path to maximize the
  effectivness of time-tiling on iterative stencil computations. Our
  approach is particularly well suited for GPUs. It does not require
  any redundant computations, it favors coalesced global-memory access
  and data reuse in shared-memory/cache, avoids thread divergence, and
  extracts a high degree of parallelism. We introduce hybrid hexagonal
  tiling, combining hexagonal tile shapes along the time (sequential)
  dimension and one spatial dimension, with classical tiling for other
  spatial dimensions. An hexagonal tile shape simultaneously enable
  parallel tile execution and reuse along the time dimension. Experimental
  results demonstrate significant performance improvements over existing
  stencil compilers.
 },
 affiliation = {PARKAS - INRIA Paris-Rocquencourt, Department of Computer
 Science and Engineering - CSE},
 type = {Rapport de recherche},
 institution = {INRIA},
 number = {RR-8339},
 year = {2013},
 month = Jul,
 pdf = {http://hal.inria.fr/hal-00848691/PDF/RR-8339.pdf},
}

@article{Verdoolaege2013PPCG,
 title = {Polyhedral parallel code generation for {CUDA}},
 author = {Verdoolaege, Sven and Juega, Juan Carlos and Cohen, Albert and
           G\'{o}mez, Jos{\'e} Ignacio and Tenllado, Christian and
           Catthoor, Francky},
 journal = {ACM Transactions on Architecture and Code Optimization},
 issue_date = {January 2013},
 volume = {9},
 number = {4},
 month = jan,
 year = {2013},
 issn = {1544-3566},
 pages = {54:1--54:23},
 doi = {10.1145/2400682.2400713},
 acmid = {2400713},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@proceedings{impact2013,
  title  = "{P}roceedings of the 3rd {I}nternational {W}orkshop on {P}olyhedral {C}ompilation {T}echniques",
  editor = "Gr{\"o}{\ss}linger, Armin and Pouchet, Louis-No{\"e}l",
  year   = 2013,
  month  = Jan,
  address = "Berlin, Germany",
  url     = "http://nbn-resolving.de/urn:nbn:de:bvb:739-opus-26930",
  note    = "http://impact.gforge.inria.fr/impact2013/"
}

@inproceedings{feld.2013.impact,
  author = "Feld, Dustin and Soddemann, Thomas and J{\"u}nger, Michael and Mallach, Sven",
  title  = "{F}acilitate {SIMD}-{C}ode-{G}eneration in the {P}olyhedral {M}odel by {H}ardware-aware {A}utomatic {C}ode-{T}ransformation",
  pages  = "45--54",
  booktitle = "{P}roceedings of the 3rd {I}nternational {W}orkshop on {P}olyhedral {C}ompilation {T}echniques",
  editor = "Gr{\"o}{\ss}linger, Armin and Pouchet, Louis-No{\"e}l",
  year   = 2013,
  month  = Jan,
  address = "Berlin, Germany",
  url = "http://impact.gforge.inria.fr/impact2013/papers/impact2013_facilitate_simd_code_generation.pdf",
  abstract = {
Although Single Instruction Multiple Data (SIMD) units are available in general
purpose processors already since the 1990s, state-of-the-art compilers are
often still not capable to fully exploit them, i.e., they may miss to achieve
the best possible performance.

We present a new hardware-aware and adaptive
loop tiling approach that is based on polyhedral transformations and explicitly
dedicated to improve on auto-vectorization. It is an extension to the tiling
algorithm implemented within the PluTo framework [4, 5]. In its default
setting, PluTo uses static tile sizes and is already capable to enable the use
of SIMD units but not primarily targeted to optimize it. We experimented with
differnt tile sizes and found a strong relationship between their choice, cache
size parameters and performance. Based on this, we designed an adaptive
procedure that speciffically tiles vectorizable loops with dynamically
calculated sizes. The blocking is automatically fitted to the amount of data
read in loop iterations, the available SIMD units and the cache sizes. The
adaptive parts are built upon straightforward calculations that are
experimentally verified and evaluated. Our results show significant
improvements in the number of instructions vectorized, cache miss rates and,
finally, running times.
  }
}

@inproceedings{yuki.2013.impact,
  author = "Yuki, Tomofumi and Rajopadhye, Sanjay",
  title  = "{M}emory {A}llocations for {T}iled {U}niform {D}ependence {P}rograms",
  pages  = "13--22",
  booktitle = "{P}roceedings of the 3rd {I}nternational {W}orkshop on {P}olyhedral {C}ompilation {T}echniques",
  editor = "Gr{\"o}{\ss}linger, Armin and Pouchet, Louis-No{\"e}l",
  year   = 2013,
  month  = Jan,
  address = "Berlin, Germany",
  url = "http://impact.gforge.inria.fr/impact2013/papers/impact2013_memory_allocations_for_tiled_uniform_dependence_programs.pdf",
  abstract = {
In this paper, we develop a series of extensions to schedule-independent
storage mapping using Quasi-Universal Occupancy Vectors (QUOVs) targeting tiled
execution of polyhedral programs. By quasi-universality, we mean that we
restrict the \universe" of the schedule to those that correspond to tiling.
This provides the following benefits: (i) the shortest QUOVs may be shorter
than the fully universal ones, (ii) the shortest QUOVs can be found without any
search, and (iii) multi-statement programs can be handled.  The resulting
storage mapping is valid for tiled execution by any tile size.
}
}

@inproceedings{fassi.2013.impact,
  author = "Fassi, Im{\`e}n and Clauss, Philippe and Kuhn, Matthieu and Slama, Yosr",
  title  = "{M}ultifor for {M}ulticore",
  pages  = "37--44",
  booktitle = "{P}roceedings of the 3rd {I}nternational {W}orkshop on {P}olyhedral {C}ompilation {T}echniques",
  editor = "Gr{\"o}{\ss}linger, Armin and Pouchet, Louis-No{\"e}l",
  year   = 2013,
  month  = Jan,
  address = "Berlin, Germany",
  url = "http://impact.gforge.inria.fr/impact2013/papers/impact2013_multifor_for_multicore.pdf",
  abstract = {
 We propose a new programming control structure called "multifor", allowing to
take advantage of parallelization models that were not naturally attainable
with the polytope model before. In a multifor-loop, several loops whose bodies
are run simultaneously can be defined. Respective iteration domains are mapped
onto each other according to a run frequency - the grain - and a relative
position - the offset -. Execution models like dataflow, stencil computations or
MapReduce can be represented onto one referential iteration domain, while still
exhibiting traditional polyhedral code analysis and transformation
opportunities. Moreover, this construct provides ways to naturally exploit
hybrid parallelization models, thus significantly improving parallelization
opportunities in the multicore era. Traditional polyhedral software tools are
used to generate the corresponding code. Additionally, a promising perspective
related to nonlinear mapping of iteration spaces is also presented, yielding to
run a loop nest inside any other one by solving the problem of inverting
"ranking Ehrhart polynomials".
  }
}

@inproceedings{verdoolaege.2013.impact,
  author = "Verdoolaege, Sven and Nikolov, Hristo and Stefanov, Todor",
  title  = "{O}n {D}emand {P}arametric {A}rray {D}ataflow {A}nalysis",
  pages  = "23--36",
  booktitle = "{P}roceedings of the 3rd {I}nternational {W}orkshop on {P}olyhedral {C}ompilation {T}echniques",
  editor = "Gr{\"o}{\ss}linger, Armin and Pouchet, Louis-No{\"e}l",
  year   = 2013,
  month  = Jan,
  address = "Berlin, Germany",
  url = "http://impact.gforge.inria.fr/impact2013/papers/impact2013_on_demand_parametric_array_dataflow_analysis.pdf",
  abstract = {
We present a novel approach for exact array data ow analysis in the presence of
constructs that are not static authorne.  The approach is similar to that of
fuzzy array data ow analysis in that it also introduces parameters that
represent information that is only available at run-time, but the parameters
have a diㄦent meaning and are analyzed before they are introduced. The
approach was motivated by our work on process networks, but should be generally
useful since fewer parameters are introduced on larger inputs. We include some
preliminary experimental results.
}
}

@inproceedings{wonnacott.2013.impact,
  author = {Wonnacott, David G. and Mills Strout, Michelle},
  title  = "{O}n the {S}calability of {L}oop {T}iling {T}echniques",
  pages  = "3--11",
  booktitle = "{P}roceedings of the 3rd {I}nternational {W}orkshop on {P}olyhedral {C}ompilation {T}echniques",
  editor = "Gr{\"o}{\ss}linger, Armin and Pouchet, Louis-No{\"e}l",
  year   = 2013,
  month  = Jan,
  address = "Berlin, Germany",
  url = "http://impact.gforge.inria.fr/impact2013/papers/impact2013_on_the_scalability_of_loop_tiling_techniques.pdf",
  abstract = {
The Polyhedral model has proven to be a valuable tool for improving memory
locality and exploiting parallelism for optimizing dense array codes. This
model is expressive enough to describe transformations of imperfectly nested
loops, and to capture a variety of program transformations, including many
approaches to loop tiling. Tools such as the highly successful PLuTo automatic
parallelizer have provided empirical confirmation of the success of
polyhedral-based optimization, through experiments in which a number of
benchmarks have been executed on machines with small- to medium-scale
parallelism.

In anticipation of ever higher degrees of parallelism, we have
explored the impact of various loop tiling strategies on the asymptotic degree
of available parallelism. In our analysis, we consider “weak scaling” as
described by Gustafson, i.e., in which the data set size grows linearly with
the number of processors available. Some, but not all, of the approaches to
tiling provide weak scaling. In particular, the tiling currently performed by
PLuTo does not scale in this sense.

In this article, we review approaches to
loop tiling in the published literature, focusing on both scalability and
implementation status. We find that fully scalable tilings are not available in
general-purpose tools, and call upon the polyhedral compilation community to
focus on questions of asymptotic scalability. Finally, we identify ongoing work
that may resolve this issue.
  }
}

@inproceedings{doerfert.2013.impact,
  author = "Doerfert, Johannes and Hammacher, Clemens and Streit, Kevin and Hack, Sebastian",
  title  = "{SP}olly: {S}peculative {O}ptimizations in the {P}olyhedral {M}odel",
  pages  = "55--60",
  booktitle = "{P}roceedings of the 3rd {I}nternational {W}orkshop on {P}olyhedral {C}ompilation {T}echniques",
  editor = "Gr{\"o}{\ss}linger, Armin and Pouchet, Louis-No{\"e}l",
  year   = 2013,
  month  = Jan,
  address = "Berlin, Germany",
  url = "http://impact.gforge.inria.fr/impact2013/papers/impact2013_spolly.pdf",
  abstract = {
The polyhedral model is only applicable to code regions that form static
control parts (SCoPs) or slight extensions thereof. To apply polyhedral
techniques to a piece of code, the compiler usually checks, by static analysis,
whether all SCoP conditions are fulfilled. However, in many codes, the compiler
fails to verify that this is the case. In this paper we investigate the
rejection causes as reported by Polly , the polyhedral optimizer of a
state-of-the-art compiler. We show that many rejections follow from the
conservative overapproximation of the employed static analyses. In SPolly , a
speculative extension of Polly, we employ the knowledge of runtime features to
supersede this overapproximation. All speculatively generated variants form
valid SCoPs and are optimizable by the facilities of Polly. Our evaluation
shows that SPolly is able to ectively widen the applicability of polyhedral
optimization. On the SPEC 2000 suite, the number of optimizable code regions is
increased by 131 percent.  In 10 out of the 31 benchmarks of the PolyBench
suite, SPolly achieves speedups of up to 11-fold as compared to plain Polly.
  }
}

@article{Grosser2012polly,
 title = {Polly -- Performing polyhedral optimizations on a low-level intermediate representation},
 author = {Grosser, Tobias and Gr{\"o}{\ss}linger, Armin and Lengauer, Christian},
 journal = {Parallel Processing Letters},
 volume = {22},
 number = {04},
 year = {2012},
 publisher = {World Scientific},
 abstract = {
The polyhedral model for loop parallelization has proved to be an effective
tool for advanced optimization and automatic parallelization of programs in
higher-level languages. Yet, to integrate such optimizations seamlessly into
production compilers, they must be performed on the compiler's internal,
low-level, intermediate representation (IR). With Polly, we present an
infrastructure for polyhedral optimizations on such an IR. We describe the
detection of program parts amenable to a polyhedral optimization (so-called
static control parts), their translation to a Z-polyhedral representation,
optimizations on this representation and the generation of optimized IR code.
Furthermore, we define an interface for connecting external optimizers and
present a novel way of using the parallelism they introduce to generate SIMD
and OpenMP code. To evaluate Polly, we compile the PolyBench 2.0 benchmarks
fully automatically with PLuTo as external optimizer and parallelizer. We can
report on significant speedups.
},
 url = {http://www.worldscientific.com/doi/abs/10.1142/S0129626412500107}
}

@inproceedings{Verdoolaege2010isl,
 author = {Verdoolaege, Sven},
 title = {isl: {A}n Integer Set Library for the Polyhedral Model},
 booktitle = {Mathematical Software (ICMS 2010)},
 year = {2010},
 pages={299--302},
 series={LNCS 4151},
 editor={Andres Iglesias and Nobuki Takayama},
 publisher = {Springer-Verlag},
}

@inproceedings{Sjodin2009design,
 title = {Design of graphite and the polyhedral compilation package},
 author = {Sj{\"o}din, Jan and Pop, Sebastian and Jagasia, Harsha and Grosser, Tobias and Pop, Antoniu and others},
 booktitle = {GCC Developers' Summit},
 year = {2009},
 abstract = "
Graphite is the loop transformation framework that was introduced in GCC 4.4.
This paper gives a detailed description of the design and future directions of
this infrastructure. Graphite uses the polyhedral model as the internal
representation (GPOLY). The plan is to create a polyhedral compilation package
(PCP) that will provide loop optimization and analysis capabilities to GCC.
This package will be separated from GIMPLE via an interface language that is
restricted to express only what GPOLY can represent. The interface language is
a set of data structures that encodes the control flow and memory accesses of a
code region. A syntax for the language is also defined to facilitate debugging
and testing.",
 url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.158.2112&rep=rep1&type=pdf},
}

@article{Baskaran2009,
 author = {Baskaran, Muthu Manikandan and Vydyanathan, Nagavijayalakshmi and
           Bondhugula, Uday Kumar Reddy and Ramanujam, J. and Rountev, Atanas
           and Sadayappan, P.},
 title = {Compiler-assisted dynamic scheduling for effective parallelization of
          loop nests on multicore processors},
 journal = {SIGPLAN Notices},
 issue_date = {April 2009},
 volume = {44},
 number = {4},
 month = feb,
 year = {2009},
 issn = {0362-1340},
 pages = {219--228},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1594835.1504209},
 doi = {10.1145/1594835.1504209},
 acmid = {1504209},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compile-time optimization, dynamic scheduling, run-time
             optimization},
 abstract = {
  Recent advances in polyhedral compilation technology have made it feasible to
  automatically transform affine sequential loop nests for tiled parallel
  execution on multi-core processors. However, for multi-statement input
  programs with statements of different dimensionalities, such as Cholesky or
  LU decomposition, the parallel tiled code generated by existing automatic
  parallelization approaches may suffer from significant load imbalance,
  resulting in poor scalability on multi-core systems. In this paper, we develop
  a completely automatic parallelization approach for transforming input affine
  sequential codes into efficient parallel codes that can be executed on a
  multi-core system in a load-balanced manner. In our approach, we employ a
  compile-time technique that enables dynamic extraction of inter-tile
  dependences at run-time, and dynamic scheduling of the parallel tiles on the
  processor cores for improved scalable execution. Our approach obviates the
  need for programmer intervention and re-writing of existing algorithms for
  efficient parallel execution on multi-cores. We demonstrate the usefulness of
  our approach through comparisons using linear algebra computations: LU and
  Cholesky decomposition.
 }
}

@article{Bondhugula2008Pluto,
 author = {Bondhugula, Uday and Hartono, Albert and Ramanujam, J. and Sadayappan, P.},
 title = {A practical automatic polyhedral parallelizer and locality optimizer},
 journal = {SIGPLAN Notices},
 volume = {43},
 number = {6},
 year = {2008},
 issn = {0362-1340},
 pages = {101--113},
 doi = {http://doi.acm.org/10.1145/1379022.1375595},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Bondhugula08cc,
    author = {Uday Bondhugula and Muthu Baskaran and Sriram
        Krishnamoorthy and J. Ramanujam and A. Rountev and P.
            Sadayappan},
    title = {Automatic Transformations for Communication-Minimized
        Parallelization and Locality Optimization in the Polyhedral Model},
    booktitle = {International Conference on Compiler Construction
        (ETAPS CC)},
    year = 2008,
    month = apr,
    url = {http://drona.csa.iisc.ernet.in/~uday/publications/uday-cc08.pdf},
    abstract = {

    The polyhedral model provides powerful abstractions to optimize loop nests with 
    regular accesses.  Affine transformations in this model capture a complex
    sequence of execution-reordering loop transformations that can
    improve performance by parallelization as well as locality
    enhancement.  Although a significant body of research has addressed
    affine scheduling and partitioning, the problem of automatically
    finding good affine transforms for communication-optimized
    coarse-grained parallelization together with locality optimization
    for the general case of arbitrarily-nested loop sequences remains a
    challenging problem.

    We propose an automatic transformation framework to
    optimize arbitrarily-nested loop sequences with affine dependences
    for parallelism and locality simultaneously. The approach finds
    good tiling hyperplanes by embedding a powerful and versatile cost
    function into an Integer Linear Programming formulation. These
    tiling hyperplanes are used for communication-minimized
    coarse-grained parallelization as well as for locality
    optimization. The approach enables the minimization of inter-tile
    communication volume in the processor space, and minimization of
    reuse distances for local execution at each node. Programs
    requiring one-dimensional versus multi-dimensional time schedules
    (with scheduling-based approaches) are all handled with the same
    algorithm.  Synchronization-free parallelism, permutable loops or
    pipelined parallelism at various levels can be detected.
    Preliminary studies of the framework show promising results.
    }
}

@PhdThesis{Bastoul2004PhD,
 author = {Bastoul, C\'{e}dric},
 title = {Improving Data Locality in Static Control Programs},
 school = {University Paris 6, Pierre et Marie Curie, France},
 month = dec,
 year = 2004,
 abstract = {
  Cache memories were invented to decouple fast processors from slow memories.
  However, this decoupling is only partial and many researchers have attempted to
  improve cache use by program optimization. Potential benefits are significant
  since both energy dissipation and performance highly depend on the traffic
  between memory levels.
  This thesis will visit most of the steps of high level transformation frameworks
  in the polyhedral model in order to improve both applicability and target code
  quality. To achieve this goal, we refine the concept of static control parts
  (SCoP) and we show experimentally that this program model is relevant to
  real-life applications. We present a transformation policy freed of classical
  limitations like considering only unimodular or invertible functions. Lastly,
  we extend the Quiller\'{e} et al. algorithm to be able to generate efficient codes
  in a reasonable amount of time. To exploit this transformation framework, we
  propose a data locality improvement method based on a singular execution scheme
  where an asymptotic evaluation of the memory traffic is possible. This information
  is used in our optimization algorithm to find the best reordering of the program
  operations, at least in an asymptotic sense. This method considers legality
  and each type of data locality as constraints whose solution is an appropriate
  transformation. The optimizer has been prototyped and tested with non-trivial
  programs. Experimental evidence shows that our framework can improve data
  locality and performance in traditionally challenging programs with e.g.
  non-perfectly nested loops, complex dependences and non-uniformly generated
  references.
 }
}

@InProceedings{Bastoul2004CLooG,
 author = {Bastoul, C\'{e}dric},
 title = {Code Generation in the Polyhedral Model Is Easier Than You Think},
 booktitle = {IEEE International Conference on Parallel Architecture
              and Compilation Techniques},
 year = 2004,
 pages = {7--16},
 month = {September},
 address = {Juan-les-Pins, France},
 abstract = {
  Many advances in automatic parallelization and optimization have been
  achieved through the polyhedral model. It has been extensively shown that this
  computational model provides convenient abstractions to reason about and apply
  program transformations. Nevertheless, the complexity of code generation has
  long been a deterrent for using polyhedral representation in optimizing
  compilers. First, code generators have a hard time coping with generated code
  size and control overhead that may spoil theoretical benefits achieved by the
  transformations. Second, this step is usually time consuming, hampering the
  integration of the polyhedral framework in production compilers or
  feedback-directed, iterative optimization schemes. Moreover, current code
  generation algorithms only cover a restrictive set of possible transformation
  functions. This paper discusses a general transformation framework able to
  deal with non-unimodular, non-invertible, non-integral or even non-uniform
  functions. It presents several improvements to a state-of-the-art code
  generation algorithm. Two directions are explored: generated code size and
  code generator efficiency. Experimental evidence proves the ability of the
  improved method to handle real-life problems.
 }
}

@article{Quillere2000,
 author = {Quiller\'{e}, Fabien and Rajopadhye, Sanjay and Wilde, Doran},
 title = {Generation of Efficient Nested Loops from Polyhedra},
 journal = {International Journal of Parallel Programming},
 volume = 28,
 number=5,
 month=oct,
 year = 2000,
 pages={469--498},
 abstract = {
  Automatic parallelization in the polyhedral model is based on affine
  transformations from an original computation domain (iteration space) to a
  target space-time domain, often with a different transformation for each
  variable. Code generation is an often ignored step in this process that has a
  significant impact on the quality of the final code. It involves making a
  trade-off between code size and control code simplification/optimization.
  Previous methods of doing code generation are based on loop splitting,
  however they have non-optimal behavior when working on parameterized
  programs. We present a general parameterized method for code generation based
  on dual representation of polyhedra. Our algorithm uses a simple recursion on
  the dimensions of the domains, and enables fine control over the tradeoff
  between code size and control overhead.
 }
}

@article{Kelly1996omega,
 title = {The Omega calculator and library, version 1.1.0},
 author = {Kelly, Wayne and Maslov, Vadim and Pugh, William and Rosser, Evan and Shpeisman, Tatiana and Wonnacott, Dave},
 year = {1996},
 url={http://www.cs.utah.edu/~mhall/cs6963s09/lectures/omega.ps}
}

@article{Feautrier1992multi,
 author = {Feautrier, Paul},
 affiliation = {Laboratoire MASI Université de Versailles St-Quentin 45 Avenue
                des Etats-Unis 78035 Versailles Cedex France},
 title = {Some efficient solutions to the affine scheduling problem. Part II.
          Multidimensional time},
 journal = {International Journal of Parallel Programming},
 publisher = {Springer Netherlands},
 issn = {0885-7458},
 keyword = {Informatique},
 pages = {389-420},
 volume = {21},
 issue = {6},
 url = {http://dx.doi.org/10.1007/BF01379404},
 note = {10.1007/BF01379404},
 year = {1992},
 abstract = {
  This paper extends the algorithms which were given in Part I to cases in which
  there is no affine schedule, i.e. to problems whose parallel complexity is
  polynomial but not linear. The natural generalization is to multidimensional
  schedules with lexicographic ordering as temporal succession. Multidimensional
  affine schedules, are, in a sense, equivalent to polynomial schedules, and are
  much easier to handle automatically. Furthermore, there is a strong connexion
  between multidimensional schedules and loop nests, which allows one to prove
  that a static control program always has a multidimensional schedule. Roughly,
  a larger dimension indicates less parallelism. In the algorithm which is
  presented here, this dimension is computed dynamically, and is just sufficient
  for scheduling the source program. The algorithm lends itself to a "divide and
  conquer" strategy. The paper gives some experimental evidence for the
  applicability, performances and limitations of the algorithm.
 }
}

@article{Feautrier1992one,
 author = {Feautrier, Paul},
 title = {Some efficient solutions to the affine scheduling problem: Part I.
          One-dimensional time},
 journal = {International Journal of Parallel Programming},
 issue_date = {Oct. 1992},
 volume = {21},
 number = {5},
 month = oct,
 year = {1992},
 issn = {0885-7458},
 pages = {313--348},
 numpages = {36},
 url = {http://dx.doi.org/10.1007/BF01407835},
 doi = {10.1007/BF01407835},
 acmid = {171448},
 publisher = {Kluwer Academic Publishers},
 address = {Norwell, MA, USA},
 keywords = {automatic parallelization, automatic systolic array design,
             scheduling},
 abstract = {
  Programs and systems of recurrence equations may be represented as
  sets of actions which are to be executed subject to precedence constraints. In
  many cases, actions may be labelled by integral vectors in some iteration
  domain, and precedence constraints may be described by affine relations. A
  schedule for such a program is a function which allows one to estimate the
  intrinsic degree of parallelism of the program and to compile a parallel
  version for multiprocessor architectures or systolic arrays. This paper deals
  with the problem of finding closed form schedules as affine or piecewise
  affine functions of the iteration vector. An efficient algorithm which
  reduces the scheduling problem to a parametric linear program of small size,
  which can be readily solved by an efficient algorithm.
 }
}

@article{Feautrier1991,
 title = {Dataflow analysis of array and scalar references},
 author = {Feautrier, Paul},
 journal = {International Journal of Parallel Programming},
 volume = {20},
 number = {1},
 pages = {23--53},
 year = {1991},
 publisher = {Springer},
 abstract = {
  Given a program written in a simple imperative language (assignment
  statements,for loops, affine indices and loop limits), this paper presents an
  algorithm for analyzing the patterns along which values flow as the execution
  proceeds. For each array or scalar reference, the result is the name and
  iteration vector of the source statement as a function of the iteration
  vector of the referencing statement. The paper discusses several applications
  of the method: conversion of a program to a set of recurrence equations,
  array and scalar expansion, program verification and parallel program
  construction.
 }
}

@article{Lamport1974,
 title = {The parallel execution of DO loops},
 author = {Lamport, Leslie},
 journal = {Communications of the ACM},
 volume = {17},
 number = {2},
 pages = {83--93},
 year = {1974},
 publisher = {ACM},
 url = {http://research.microsoft.com/en-us/um/people/lamport/pubs/do-loops.pdf},
 abstract = {
  Methods are developed for the parallel execution of different iterations of a
  DO loop. Both asynchronous multiprocessor computers and array computers are
  considered. Practical application to the design of compilers for such
  computers is discussed.
 }
}

@article{Karp1967,
 title = {The organization of computations for uniform recurrence equations},
 author = {Karp, Richard M and Miller, Raymond E and Winograd, Shmuel},
 journal = {Journal of the ACM},
 volume = {14},
 number = {3},
 pages = {563--590},
 year = {1967},
 publisher = {ACM},
 url = {http://dl.acm.org/citation.cfm?id=321418},
 abstract = {
  A set equations in the quantities ai(p), where i = 1, 2, · · ·, m and p ranges
  over a set R of lattice points in n-space, is called a system of uniform
  recurrence equations if the following property holds: If p and q are in R and
  w is an integer n-vector, then ai(p) depends directly on aj(p - w) if and
  only if ai(q) depends directly on aj(q - w). Finite-difference approximations
  to systems of partial differential equations typically lead to such recurrence
  equations. The structure of such a system is specified by a dependence graph G
  having m vertices, in which the directed edges are labeled with integer
  n-vectors. For certain choices of the set R, necessary and sufficient
  conditions on G are given for the existence of a schedule to compute all the
  quantities ai(p) explicitly from their defining equations. Properties of such
  schedules, such as the degree to which computation can proceed “in parallel,”
  are characterized. These characterizations depend on a certain iterative
  decomposition of a dependence graph into subgraphs. Analogous results
  concerning implicit schedules are also given.
 }
}
