@Comment{{
We try to keep the formatting in this bibtex file consistent. Please
try to follow the following style guide.

- Order: The entries of this file are ordered by year of appearance and
          then by the bibtex tags (newest entries at the top).
- Keys:  Use the style firstauthor.lastname + year + optional-tag.
         E.g. [Feautrier1992multi]
- '{}': Use a single pair of braces and embrace individual words/letters
         that should always remain uppercase.
- Abbreviations: Do not abbreviate conferences and journal names.
- Abstracts: Include abstracts, if available.
- ACM style: For all remaining style issues, we to follow the style used by ACM
         (see e.g., Baskaran2009)

!! The style rules are necessarily incomplete, if you would like to improve
   the style of this file, feel free to provide a patch that both extends
   the style guide and fixes the existing entries.
}}

@techreport{Grosser2013Promises,
 hal_id = {hal-00848691},
 url = {http://hal.inria.fr/hal-00848691},
 title = {{The Promises of Hybrid Hexagonal/Classical Tiling for GPU}},
 author = {Grosser, Tobias and Verdoolaege, Sven and Cohen, Albert and Sadayappan, P.},
 abstract = {{
  Time-tiling is necessary for efficient execution of iterative
  stencil computations. But the usual hyper-rectangular tiles cannot
  be used because of positive/negative dependence distances along the
  stencil's spatial dimensions. Several prior efforts have addressed
  this issue. However, known techniques trade enhanced data reuse
  for other causes of inefficiency, such as unbalanced parallelism,
  redundant computations, or increased control flow overhead incompatible
  with efficient GPU execution. We explore a new path to maximize the
  effectivness of time-tiling on iterative stencil computations. Our
  approach is particularly well suited for GPUs. It does not require
  any redundant computations, it favors coalesced global-memory access
  and data reuse in shared-memory/cache, avoids thread divergence, and
  extracts a high degree of parallelism. We introduce hybrid hexagonal
  tiling, combining hexagonal tile shapes along the time (sequential)
  dimension and one spatial dimension, with classical tiling for other
  spatial dimensions. An hexagonal tile shape simultaneously enable
  parallel tile execution and reuse along the time dimension. Experimental
  results demonstrate significant performance improvements over existing
  stencil compilers.
 }},
 affiliation = {PARKAS - INRIA Paris-Rocquencourt, Department of Computer
 Science and Engineering - CSE},
 type = {Rapport de recherche},
 institution = {INRIA},
 number = {RR-8339},
 year = {2013},
 month = Jul,
 pdf = {http://hal.inria.fr/hal-00848691/PDF/RR-8339.pdf},
}

@article{Verdoolaege2013PPCG,
 title = {Polyhedral parallel code generation for {CUDA}},
 author = {Verdoolaege, Sven and Juega, Juan Carlos and Cohen, Albert and
           G\'{o}mez, Jos{\'e} Ignacio and Tenllado, Christian and
           Catthoor, Francky},
 journal = {ACM Transactions on Architecture and Code Optimization},
 issue_date = {January 2013},
 volume = {9},
 number = {4},
 month = jan,
 year = {2013},
 issn = {1544-3566},
 pages = {54:1--54:23},
 doi = {10.1145/2400682.2400713},
 acmid = {2400713},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Verdoolaege2010isl,
 author = {Verdoolaege, Sven},
 title = {isl: {A}n Integer Set Library for the Polyhedral Model},
 booktitle = {Mathematical Software (ICMS 2010)},
 year = {2010},
 pages={299--302},
 series={LNCS 4151},
 editor={Andres Iglesias and Nobuki Takayama},
 publisher = {Springer-Verlag},
}

@article{Baskaran2009,
 author = {Baskaran, Muthu Manikandan and Vydyanathan, Nagavijayalakshmi and
           Bondhugula, Uday Kumar Reddy and Ramanujam, J. and Rountev, Atanas
           and Sadayappan, P.},
 title = {Compiler-assisted dynamic scheduling for effective parallelization of
          loop nests on multicore processors},
 journal = {SIGPLAN Not.},
 issue_date = {April 2009},
 volume = {44},
 number = {4},
 month = feb,
 year = {2009},
 issn = {0362-1340},
 pages = {219--228},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1594835.1504209},
 doi = {10.1145/1594835.1504209},
 acmid = {1504209},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compile-time optimization, dynamic scheduling, run-time
             optimization},
 abstract = {
  Recent advances in polyhedral compilation technology have made it feasible to
  automatically transform affine sequential loop nests for tiled parallel
  execution on multi-core processors. However, for multi-statement input
  programs with statements of different dimensionalities, such as Cholesky or
  LU decomposition, the parallel tiled code generated by existing automatic
  parallelization approaches may suffer from significant load imbalance,
  resulting in poor scalability on multi-core systems. In this paper, we develop
  a completely automatic parallelization approach for transforming input affine
  sequential codes into efficient parallel codes that can be executed on a
  multi-core system in a load-balanced manner. In our approach, we employ a
  compile-time technique that enables dynamic extraction of inter-tile
  dependences at run-time, and dynamic scheduling of the parallel tiles on the
  processor cores for improved scalable execution. Our approach obviates the
  need for programmer intervention and re-writing of existing algorithms for
  efficient parallel execution on multi-cores. We demonstrate the usefulness of
  our approach through comparisons using linear algebra computations: LU and
  Cholesky decomposition.
 }
}

@article{Bondhugula2008Pluto,
 author = {Bondhugula, Uday and Hartono, Albert and Ramanujam, J. and Sadayappan, P.},
 title = {A practical automatic polyhedral parallelizer and locality optimizer},
 journal = {SIGPLAN Notices},
 volume = {43},
 number = {6},
 year = {2008},
 issn = {0362-1340},
 pages = {101--113},
 doi = {http://doi.acm.org/10.1145/1379022.1375595},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@PhdThesis{Bastoul2004PhD,
 author = {Bastoul, C\'{e}dric},
 title = {Improving Data Locality in Static Control Programs},
 school = {University Paris 6, Pierre et Marie Curie, France},
 month = dec,
 year = 2004,
 abstract = {
  Cache memories were invented to decouple fast processors from slow memories.
  However, this decoupling is only partial and many researchers have attempted to
  improve cache use by program optimization. Potential benefits are significant
  since both energy dissipation and performance highly depend on the traffic
  between memory levels.
  This thesis will visit most of the steps of high level transformation frameworks
  in the polyhedral model in order to improve both applicability and target code
  quality. To achieve this goal, we refine the concept of static control parts
  (SCoP) and we show experimentally that this program model is relevant to
  real-life applications. We present a transformation policy freed of classical
  limitations like considering only unimodular or invertible functions. Lastly,
  we extend the Quiller\'{e} et al. algorithm to be able to generate efficient codes
  in a reasonable amount of time. To exploit this transformation framework, we
  propose a data locality improvement method based on a singular execution scheme
  where an asymptotic evaluation of the memory traffic is possible. This information
  is used in our optimization algorithm to find the best reordering of the program
  operations, at least in an asymptotic sense. This method considers legality
  and each type of data locality as constraints whose solution is an appropriate
  transformation. The optimizer has been prototyped and tested with non-trivial
  programs. Experimental evidence shows that our framework can improve data
  locality and performance in traditionally challenging programs with e.g.
  non-perfectly nested loops, complex dependences and non-uniformly generated
  references.
 }
}

@InProceedings{Bastoul2004CLooG,
 author = {Bastoul, C\'{e}dric},
 title = {Code Generation in the Polyhedral Model Is Easier Than You Think},
 booktitle = {PACT'13 IEEE International Conference on Parallel Architecture
              and Compilation Techniques},
 year = 2004,
 pages = {7--16},
 month = {September},
 address = {Juan-les-Pins, France},
 abstract = {
  Many advances in automatic parallelization and optimization have been
  achieved through the polyhedral model. It has been extensively shown that this
  computational model provides convenient abstractions to reason about and apply
  program transformations. Nevertheless, the complexity of code generation has
  long been a deterrent for using polyhedral representation in optimizing
  compilers. First, code generators have a hard time coping with generated code
  size and control overhead that may spoil theoretical benefits achieved by the
  transformations. Second, this step is usually time consuming, hampering the
  integration of the polyhedral framework in production compilers or
  feedback-directed, iterative optimization schemes. Moreover, current code
  generation algorithms only cover a restrictive set of possible transformation
  functions. This paper discusses a general transformation framework able to
  deal with non-unimodular, non-invertible, non-integral or even non-uniform
  functions. It presents several improvements to a state-of-the-art code
  generation algorithm. Two directions are explored: generated code size and
  code generator efficiency. Experimental evidence proves the ability of the
  improved method to handle real-life problems.
 }
}

@article{Quillere2000,
 author = {Quiller\'{e}, Fabien and Rajopadhye, Sanjay and Wilde, Doran},
 title = {Generation of Efficient Nested Loops from Polyhedra},
 journal = {International Journal of Parallel Programming},
 volume = 28,
 number=5,
 month=oct,
 year = 2000,
 pages={469--498},
 abstract = {
  Automatic parallelization in the polyhedral model is based on affine
  transformations from an original computation domain (iteration space) to a
  target space-time domain, often with a different transformation for each
  variable. Code generation is an often ignored step in this process that has a
  significant impact on the quality of the final code. It involves making a
  trade-off between code size and control code simplification/optimization.
  Previous methods of doing code generation are based on loop splitting,
  however they have non-optimal behavior when working on parameterized
  programs. We present a general parameterized method for code generation based
  on dual representation of polyhedra. Our algorithm uses a simple recursion on
  the dimensions of the domains, and enables fine control over the tradeoff
  between code size and control overhead.
 }
}

@article{Feautrier1992multi,
 author = {Feautrier, Paul},
 affiliation = {Laboratoire MASI Université de Versailles St-Quentin 45 Avenue
                des Etats-Unis 78035 Versailles Cedex France},
 title = {Some efficient solutions to the affine scheduling problem. Part II.
          Multidimensional time},
 journal = {International Journal of Parallel Programming},
 publisher = {Springer Netherlands},
 issn = {0885-7458},
 keyword = {Informatique},
 pages = {389-420},
 volume = {21},
 issue = {6},
 url = {http://dx.doi.org/10.1007/BF01379404},
 note = {10.1007/BF01379404},
 year = {1992},
 abstract = {
  This paper extends the algorithms which were given in Part I to cases in which
  there is no affine schedule, i.e. to problems whose parallel complexity is
  polynomial but not linear. The natural generalization is to multidimensional
  schedules with lexicographic ordering as temporal succession. Multidimensional
  affine schedules, are, in a sense, equivalent to polynomial schedules, and are
  much easier to handle automatically. Furthermore, there is a strong connexion
  between multidimensional schedules and loop nests, which allows one to prove
  that a static control program always has a multidimensional schedule. Roughly,
  a larger dimension indicates less parallelism. In the algorithm which is
  presented here, this dimension is computed dynamically, and is just sufficient
  for scheduling the source program. The algorithm lends itself to a "divide and
  conquer" strategy. The paper gives some experimental evidence for the
  applicability, performances and limitations of the algorithm.
 }
}

@article{Feautrier1992one,
 author = {Feautrier, Paul},
 title = {Some efficient solutions to the affine scheduling problem: Part I.
          One-dimensional time},
 journal = {International Journal of Parallel Programming},
 issue_date = {Oct. 1992},
 volume = {21},
 number = {5},
 month = oct,
 year = {1992},
 issn = {0885-7458},
 pages = {313--348},
 numpages = {36},
 url = {http://dx.doi.org/10.1007/BF01407835},
 doi = {10.1007/BF01407835},
 acmid = {171448},
 publisher = {Kluwer Academic Publishers},
 address = {Norwell, MA, USA},
 keywords = {automatic parallelization, automatic systolic array design,
             scheduling},
 abstract = {
  Programs and systems of recurrence equations may be represented as
  sets of actions which are to be executed subject to precedence constraints. In
  many cases, actions may be labelled by integral vectors in some iteration
  domain, and precedence constraints may be described by affine relations. A
  schedule for such a program is a function which allows one to estimate the
  intrinsic degree of parallelism of the program and to compile a parallel
  version for multiprocessor architectures or systolic arrays. This paper deals
  with the problem of finding closed form schedules as affine or piecewise
  affine functions of the iteration vector. An efficient algorithm which
  reduces the scheduling problem to a parametric linear program of small size,
  which can be readily solved by an efficient algorithm.
 }
}

@article{Feautrier1991,
 title = {Dataflow analysis of array and scalar references},
 author = {Feautrier, Paul},
 journal = {International Journal of Parallel Programming},
 volume = {20},
 number = {1},
 pages = {23--53},
 year = {1991},
 publisher = {Springer},
 abstract = {
  Given a program written in a simple imperative language (assignment
  statements,for loops, affine indices and loop limits), this paper presents an
  algorithm for analyzing the patterns along which values flow as the execution
  proceeds. For each array or scalar reference, the result is the name and
  iteration vector of the source statement as a function of the iteration
  vector of the referencing statement. The paper discusses several applications
  of the method: conversion of a program to a set of recurrence equations,
  array and scalar expansion, program verification and parallel program
  construction.
 }
}

@article{Lamport1974,
 title = {The parallel execution of DO loops},
 author = {Lamport, Leslie},
 journal = {Communications of the ACM},
 volume = {17},
 number = {2},
 pages = {83--93},
 year = {1974},
 publisher = {ACM},
 url = {http://research.microsoft.com/en-us/um/people/lamport/pubs/do-loops.pdf},
 abstract = {
  Methods are developed for the parallel execution of different iterations of a
  DO loop. Both asynchronous multiprocessor computers and array computers are
  considered. Practical application to the design of compilers for such
  computers is discussed.
 }
}

@article{Karp1967,
 title = {The organization of computations for uniform recurrence equations},
 author = {Karp, Richard M and Miller, Raymond E and Winograd, Shmuel},
 journal = {Journal of the ACM (JACM)},
 volume = {14},
 number = {3},
 pages = {563--590},
 year = {1967},
 publisher = {ACM},
 url = {http://dl.acm.org/citation.cfm?id=321418},
 abstract = {
  A set equations in the quantities ai(p), where i = 1, 2, · · ·, m and p ranges
  over a set R of lattice points in n-space, is called a system of uniform
  recurrence equations if the following property holds: If p and q are in R and
  w is an integer n-vector, then ai(p) depends directly on aj(p - w) if and
  only if ai(q) depends directly on aj(q - w). Finite-difference approximations
  to systems of partial differential equations typically lead to such recurrence
  equations. The structure of such a system is specified by a dependence graph G
  having m vertices, in which the directed edges are labeled with integer
  n-vectors. For certain choices of the set R, necessary and sufficient
  conditions on G are given for the existence of a schedule to compute all the
  quantities ai(p) explicitly from their defining equations. Properties of such
  schedules, such as the degree to which computation can proceed “in parallel,”
  are characterized. These characterizations depend on a certain iterative
  decomposition of a dependence graph into subgraphs. Analogous results
  concerning implicit schedules are also given.
 }
}
